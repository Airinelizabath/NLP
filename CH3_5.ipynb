{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH3-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4nx5O7rZj5AZMg3NvvaCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Airinelizabath/NLP/blob/main/CH3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QdIDtlbbT95"
      },
      "source": [
        "# **Chapter: 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6nR5IhFdGyJ"
      },
      "source": [
        "import python_utils\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSyG8UEX7NGR"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a52O90K7_Qt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "391caf23-c415-4b9b-bd5a-38a99ea08f96"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckxd9sCyG44E"
      },
      "source": [
        " from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJBqHjXbbVt"
      },
      "source": [
        "1. Define a string s = 'colorless' . Write a Python statement that changes this to “colourless” using only the slice and concatenation operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS_zP4LHZbPj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2560ce21-415a-4fc9-ce51-53fd48da44ba"
      },
      "source": [
        "s='colorless'\n",
        "s[:4]+'u'+s[4:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'colourless'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fUo4IvboB0"
      },
      "source": [
        "2. We can use the slice notation to remove morphological endings on words. For\n",
        "example, `'dogs'[:-1]` removes the last character of dogs , leaving dog . Use slice\n",
        "notation to remove the affixes from these words (we’ve inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es , run-ning , nation-ality , un-do , pre-heat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwyRXZU7eAsZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "88887cb6-d5a4-4dcd-dad3-58dbb8c71b66"
      },
      "source": [
        "print('dishes'[:-2])\n",
        "print('running'[:-4])\n",
        "print('nationality'[:-5])\n",
        "print('undo'[:-2])\n",
        "print('preheat'[:-4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dish\n",
            "run\n",
            "nation\n",
            "un\n",
            "pre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlFtWh5SbuHu"
      },
      "source": [
        "3. We saw how we can generate an IndexError by indexing beyond the end of a\n",
        "string. Is it possible to construct an index that goes too far to the left, before the\n",
        "start of the string?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl5eKsdFj-j6"
      },
      "source": [
        "Yes, that is possible. Given a string s, s[-(len(s)+1)] will generate an IndexError since it goes too far to the left from the right."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqxwLZzabyeX"
      },
      "source": [
        "4. We can specify a “step” size for the slice. The following returns every second\n",
        "character within the slice: monty[6:11:2] . It also works in the reverse direction:\n",
        "monty[10:5:-2] . Try these for yourself, and then experiment with different step\n",
        "values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7joUndQ6kRsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "86a50c4c-8380-4a39-90a1-712e2d85079d"
      },
      "source": [
        "monty='python monty'\n",
        "print(monty[6:11:2])\n",
        "print(monty[10:5:-2])\n",
        "print(monty[:10:3])\n",
        "print(monty[::-4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ot\n",
            "to \n",
            "ph n\n",
            "ymh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWcsCzcgb18K"
      },
      "source": [
        "5. What happens if you ask the interpreter to evaluate monty[ : :-1] ? Explain why\n",
        "this is a reasonable result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd9V88Zma0uW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2de43642-9f35-4e31-86f9-3988f1e524bd"
      },
      "source": [
        "monty[ : :-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ytnom nohtyp'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4objLZolFsS"
      },
      "source": [
        "It allows slicing the string from start to last in reverse order skipping one character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtO6FrYqb5fi"
      },
      "source": [
        "6. Describe the class of strings matched by the following regular expressions:  \n",
        "a. [a-zA-Z]+  \n",
        "b. [A-Z][a-z]*  \n",
        "c. p[aeiou]{,2}t  \n",
        "d. \\d+(\\.\\d+)?  \n",
        "e. ([^aeiou][aeiou][^aeiou])*  \n",
        "f. \\w+|[^\\w\\s]+  \n",
        "Test your answers using nltk.re_show() ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZMcCZYGlwya"
      },
      "source": [
        "a. All words with atleast one alphabet irrespctive of capitalized and non-capitalized chartacters.  \n",
        "b. All possible combination of words atleast with one character and starting with a capital letter.  \n",
        "c. All words starting with p followed by not more than 2 vowels and followed by t.  \n",
        "d. All positive decimal numbers  \n",
        "e. 0 or more words which are have non-vowels followed by a vowel and then a non-vowel form throughout the word.   \n",
        "f. one or more word characters **or** doesn't contaion a word character or white space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anxM86iKMshw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "5321962e-7a17-4a7c-dbae-f187d21a3dff"
      },
      "source": [
        "txt='Shoes on, get up in the mornCup of milk, let\\'s rock and rollKing Kong, kick the drum, rolling on like a rolling stone'\n",
        "nltk.re_show('[a-zA-Z]+',txt)\n",
        "nltk.re_show('[A-Z][a-z]*',txt)\n",
        "nltk.re_show('p[aeiou]{,2}t','pet pot powder plastic bottle')\n",
        "nltk.re_show('\\d+(.\\d+)?','1.2 12 86 2.e5 nbsa')\n",
        "nltk.re_show('([^aeiou][aeiou][^aeiou])*','pet pot powder plastic bottle')\n",
        "nltk.re_show('\\w+|[^\\w\\s]+',txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{Shoes} {on}, {get} {up} {in} {the} {mornCup} {of} {milk}, {let}'{s} {rock} {and} {rollKing} {Kong}, {kick} {the} {drum}, {rolling} {on} {like} {a} {rolling} {stone}\n",
            "{Shoes} on, get up in the morn{Cup} of milk, let's rock and roll{King} {Kong}, kick the drum, rolling on like a rolling stone\n",
            "{pet} {pot} powder plastic bottle\n",
            "{1.2} {12 86} {2}.e{5} nbsa\n",
            "{pet} {pot} {powder} {}p{lastic} {bot}t{}l{}e{}\n",
            "{Shoes} {on}{,} {get} {up} {in} {the} {mornCup} {of} {milk}{,} {let}{'}{s} {rock} {and} {rollKing} {Kong}{,} {kick} {the} {drum}{,} {rolling} {on} {like} {a} {rolling} {stone}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zYqkBd_cX9Y"
      },
      "source": [
        "7. Write regular expressions to match the following classes of strings:  \n",
        "a. A single determiner (assume that a, an, and the are the only determiners)  \n",
        "b. An arithmetic expression using integers, addition, and multiplication, such as\n",
        "2*3+8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD1qlnDBbzQ4"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "a. r'\\b(an?|the)\\b'\n",
        "b. r'-?[0-9]+([\\+\\/\\-\\*]-?[0-9]*)*' r'(/d[\\+\\*]/d)+'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ_ZRuz7eh98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7a98ee9-0a2a-4b5e-b774-4473ca44979a"
      },
      "source": [
        "re.findall('(an?|the)',txt)\n",
        "#re.findall('([0-9]+[\\+\\/\\-\\*][0-9]+[\\+\\/\\-\\*][0-9])','3+6-1 gvhavshs 32+45-9')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'an', 'the', 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfOWagPecnBh"
      },
      "source": [
        "8. Write a utility function that takes a URL as its argument, and returns the contents\n",
        "of the URL, with all HTML markup removed. Use urllib.urlopen to access the\n",
        "contents of the URL, e.g.:   \n",
        "        `raw_contents = urllib.urlopen('http://www.nltk.org/').read()`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlLGJVWa4Bet"
      },
      "source": [
        "from urllib import request\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DMTJrBwmBD9"
      },
      "source": [
        "def ret_url(URL):\n",
        "  raw_contents = request.urlopen(URL).read().decode('utf8')\n",
        "  raw_contents = BeautifulSoup(raw_contents).get_text()\n",
        "  tokens = nltk.tokenize.word_tokenize(raw_contents)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8GCFvNOoTgO"
      },
      "source": [
        "ret_url('http://www.nltk.org/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfr1Gyd7m_ir"
      },
      "source": [
        "9. Save some text into a file corpus.txt. Define a function load(f) that reads from\n",
        "the file named in its sole argument, and returns a string containing the text of the\n",
        "file.  \n",
        "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\n",
        "kinds of punctuation in this text. Use one multiline regular expression inline\n",
        "comments, using the verbose flag (?x) .   \n",
        "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\n",
        "kinds of expressions: monetary amounts; dates; names of people and\n",
        "organizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-_TsrlgrZDN"
      },
      "source": [
        "#a\n",
        "def load(f):\n",
        "  file = open(f)\n",
        "  raw=file.read()\n",
        "  return nltk.word_tokenize(raw)\n",
        "  \n",
        "load('corpus.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNFCK-4f9tST"
      },
      "source": [
        "#b\n",
        "def punct(f):\n",
        "  file = open(f)\n",
        "  raw = file.read()\n",
        "  pattern=r'''(?x)        \n",
        "        [,\\.]                 # comma, period\n",
        "      | [\\[\\](){}<>]          # brackets () {} [] <>\n",
        "      | ['\"“]                 # quotation marks\n",
        "      | [?!]                  # question mark and exclamation mark\n",
        "      | [:;]                  # colon and semicolon\n",
        "      | \\.\\.\\.                # ellipsis\n",
        "    '''\n",
        "  return nltk.regexp_tokenize(raw, pattern)\n",
        "\n",
        "punct('corpus.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkLEx1wAs8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "443960eb-bb1e-448d-e639-8898e709d154"
      },
      "source": [
        "def load_monetary(f):\n",
        "  file = open(f)\n",
        "  raw = file.read()\n",
        "  pattern = r'''(?x)\n",
        "      \\$\\d+(?:,\\d+)*(?:\\.\\d+)?      # USD\n",
        "    | £\\d+(?:,\\d+)*(?:\\.\\d+)?       # GBP\n",
        "    | ￥\\d+(?:\\.\\d+)?               # CNY\n",
        "  '''\n",
        "  return nltk.regexp_tokenize(raw, pattern)\n",
        "\n",
        "load_monetary('corpus.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$1,000', '£999.99', '￥1000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-JrH1uB9ED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6818cfdd-b52e-4b6d-db29-5d892620e5f8"
      },
      "source": [
        "def load_date(f):\n",
        "    file = open(f)\n",
        "    raw = file.read()\n",
        "    pattern = r'''(?x)\n",
        "        \\d{,4}[/\\.-]\\d{1,2}[/\\.-]\\d{1,2}       # big-endian, e.g., 1996-10-23, 1996.10.23, 1996/10/23\n",
        "      | \\d{1,2}[/\\.-]\\d{1,2}[/\\.-]\\d{,4}       # little-endian or middle-endian, dd/mm/yyyy or mm/dd/yyyy \n",
        "    '''    \n",
        "    return nltk.regexp_tokenize(raw, pattern)\n",
        "load_date('corpus.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2018-08-06', '2018.08.06', '08/06/20', '06/08/20', '06/08/18', '06-08-20']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DdYPT5snIjT"
      },
      "source": [
        "10. Rewrite the following loop as a list comprehension:\n",
        "```\n",
        " sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
        " result = []\n",
        " for word in sent:\n",
        "      word_len = (word, len(word))\n",
        "      result.append(word_len)  \n",
        " result\n",
        " [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmkjw-bCNYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "27de9529-c929-4a3f-efb7-c0cbabef66d2"
      },
      "source": [
        "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
        "[(word,len(word)) for word in sent]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 3),\n",
              " ('dog', 3),\n",
              " ('gave', 4),\n",
              " ('John', 4),\n",
              " ('the', 3),\n",
              " ('newspaper', 9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOsLMEBQoBYy"
      },
      "source": [
        "11. Define a string raw containing a sentence of your own choosing. Now, split raw\n",
        "on some character other than space, such as 's' ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGIpz5FDM9F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "3cbd9069-425f-4ac0-b930-e0ff72ecd2c3"
      },
      "source": [
        "raw='Purple is the last color of the rainbow colors. Purple means I will trust and love you for a long time. I just made it up.'\n",
        "raw.split('s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Purple i',\n",
              " ' the la',\n",
              " 't color of the rainbow color',\n",
              " '. Purple mean',\n",
              " ' I will tru',\n",
              " 't and love you for a long time. I ju',\n",
              " 't made it up.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hgixF-hoDVs"
      },
      "source": [
        "12. Write a for loop to print out the characters of a string, one per line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD_3Ej8dg0Pf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "ded0aa30-4313-4a33-d671-0ea4130051ac"
      },
      "source": [
        "raw='Borahae'\n",
        "for l in raw:\n",
        "  print(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "o\n",
            "r\n",
            "a\n",
            "h\n",
            "a\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcBf-87-oGvx"
      },
      "source": [
        "13. What is the difference between calling split on a string with no argument and\n",
        "one with ' ' as the argument, e.g., sent.split() versus sent.split(' ') ? What\n",
        "happens when the string being split contains tab characters, consecutive space\n",
        "characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\n",
        "enter a tab character.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_UX9MQOhWE3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b1d9ab9d-b66d-44b5-8088-182e7339f7b0"
      },
      "source": [
        "raw='''Purple is the last color of the rainbow colors.\n",
        " Purple means I will trust and love you for a long time. \n",
        "      I just made it up.'''\n",
        "print(raw.split())\n",
        "print(raw.split(' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Purple', 'is', 'the', 'last', 'color', 'of', 'the', 'rainbow', 'colors.', 'Purple', 'means', 'I', 'will', 'trust', 'and', 'love', 'you', 'for', 'a', 'long', 'time.', 'I', 'just', 'made', 'it', 'up.']\n",
            "['Purple', 'is', 'the', 'last', 'color', 'of', 'the', 'rainbow', 'colors.\\n', 'Purple', 'means', 'I', 'will', 'trust', 'and', 'love', 'you', 'for', 'a', 'long', 'time.', '\\n', '', '', '', '', '', 'I', 'just', 'made', 'it', 'up.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2fb3cKoKGJ"
      },
      "source": [
        "14.Create a variable words containing a list of words. Experiment with\n",
        "words.sort() and sorted(words) . What is the difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkvbuESrluwc"
      },
      "source": [
        "words=['I', 'just', 'made', 'it', 'up.']\n",
        "words.sort()    # does not print the output upon function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snmXjhbl1_6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98defb9f-2c00-44ac-cf30-403f209e2304"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'it', 'just', 'made', 'up.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4fX-1Fr2Bp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea164cb7-dd0c-45e5-a4ee-aa96512f0d9b"
      },
      "source": [
        "sorted(words)   # outputs the sorted list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'it', 'just', 'made', 'up.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZPcG7NfoPnD"
      },
      "source": [
        "15. Explore the difference between strings and integers by typing the following at a\n",
        "Python prompt: \"3\" * 7 and 3 * 7 . Try converting between strings and integers\n",
        "using int(\"3\") and str(3) ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-PsyS0b2Sv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0a510a6-a25c-419b-f3bf-32f683d7219e"
      },
      "source": [
        "\"3\"*7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3333333'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnVPtgNv2cl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a399061c-dc93-4842-9aac-f75f73d77275"
      },
      "source": [
        "3*7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnPTrv3i2hav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fb8dec3a-2e31-4c19-8d8e-6a0dcb51e2eb"
      },
      "source": [
        "print(int(\"3\"), type(int(\"3\")))\n",
        "print(str(3), type(str(3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 <class 'int'>\n",
            "3 <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IxzMefPoQz5"
      },
      "source": [
        "16. Earlier, we asked you to use a text editor to create a file called test.py, containing\n",
        "the single line monty = 'Monty Python' . If you haven’t already done this (or can’t\n",
        "find the file), go ahead and do it now. Next, start up a new session with the Python nterpreter, and enter the expression monty at the prompt. You will get an error\n",
        "from the interpreter. Now, try the following (note that you have to leave off\n",
        "the .py part of the filename):\n",
        "\n",
        "\n",
        "```\n",
        " >>> from test import msg\n",
        " >>> msg\n",
        "\n",
        "```\n",
        "This time, Python should return with a value. You can also try import test , in\n",
        "which case Python should be able to evaluate the expression test.monty at the\n",
        "prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVKYMpjK2sVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "16b11742-bafb-48c4-f772-c146d91e735f"
      },
      "source": [
        "monty"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d4cc90107335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'monty' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6v1VD1z4GRl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f391d54-2c60-4a89-c01a-08aeb7203cba"
      },
      "source": [
        "from test import monty\n",
        "monty"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Monty Python'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r6i4gv8ooqI"
      },
      "source": [
        "17. What happens when the formatting strings %6s and %-6s are used to display\n",
        "strings that are longer than six characters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g85n5tQv4QAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cee2fd24-8693-42c6-816e-bae767d85e43"
      },
      "source": [
        "raw='MontyPython'\n",
        "print('%6s'%raw)\n",
        "print('%-6s'%raw)\n",
        " #no issues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MontyPython\n",
            "MontyPython\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp_ifyqYot3V"
      },
      "source": [
        "18. Read in some text from a corpus, tokenize it, and print the list of all wh-word\n",
        "types that occur. (wh-words in English are used in questions, relative clauses, and\n",
        "exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
        "duplicated in this list, because of the presence of case distinctions or punctuation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_JDMJ6k4QEP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d5450bea-e932-4011-be4e-c305d472f129"
      },
      "source": [
        "f = 'corpus.txt'\n",
        "file = open(f)\n",
        "raw = file.read()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "print([wh for wh in tokens if wh.lower().startswith('wh')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who', 'where', 'who', 'when', 'when', 'what', 'who', 'what', 'who', 'who', 'who', 'What', 'What', 'who', 'where']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzzAGjE7A3BQ"
      },
      "source": [
        "There are duplicates in the list due to case distinction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PgfBmIPowDy"
      },
      "source": [
        "19. Create a file consisting of words and (made up) frequencies, where each line\n",
        "consists of a word, the space character, and a positive integer, e.g., fuzzy 53 . Read\n",
        "the file into a Python list using open(filename).readlines() . Next, break each line\n",
        "into its two fields using split() , and convert the number into an integer using\n",
        "int() . The result should be a list of the form: [['fuzzy', 53], ...] ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGW4aN3ZBFpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "b920d13f-88f9-485b-ce3a-fb6f6f787a35"
      },
      "source": [
        "filename = 'test2.txt'\n",
        "lines = open(filename).readlines()\n",
        "[[item.split(' ')[0],int(item.split(' ')[1][:-1])] for item in lines]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Dynamite', 514],\n",
              " ['Boy_with_Luv', 997],\n",
              " ['On', 276],\n",
              " ['Idol', 770],\n",
              " ['DNA', 1115],\n",
              " ['fake_love', 793],\n",
              " ['stay_gold', 128],\n",
              " ['dope', 670],\n",
              " ['daechwita', 152],\n",
              " ['fire', 795],\n",
              " ['blood_sweat_and_tears', 646],\n",
              " ['black_swan', 185],\n",
              " ['Run', 214],\n",
              " ['Save_me', 53]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8V3fuDyoy2F"
      },
      "source": [
        "20. Write code to access a favorite web page and extract some text from it. For\n",
        "example, access a weather site and extract the forecast top temperature for your\n",
        "town or city today."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrhGILfNgXA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be896f72-347c-4dcf-ff5e-da163529a389"
      },
      "source": [
        "#URL='https://darksky.net/forecast/10.0826,76.3833/ca24/en'\n",
        "URL='https://www.msn.com/en-us/Weather/today/nilamburkeralaindia/we-city?q=nilambur-kerala&form=PRWLAS&iso=IN&el=LVbO9jkVG3sQm4cHgW0fig%3d%3d'\n",
        "raw_contents = request.urlopen(URL).read().decode('utf8')\n",
        "raw_contents = BeautifulSoup(raw_contents).get_text()\n",
        "raw_contents = [item[0:2] +'°F' for item in re.findall(r'([0-9]+\\n°F\\n°C)',raw_contents)]\n",
        "raw_contents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['79°F']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzdN-in1o73f"
      },
      "source": [
        "21. Write a function **unknown( )** that takes a URL as its argument, and returns a list\n",
        "of unknown words that occur on that web page. In order to do this, extract all\n",
        "substrings consisting of lowercase letters **(using re.findall( ) )** and remove any\n",
        "items from this set that occur in the Words Corpus **( nltk.corpus.words )**. Try to\n",
        "categorize these words manually and discuss your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIfyRx2VJtgu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "da9c3f0d-091e-47a4-cb29-c9266c222abc"
      },
      "source": [
        "def unknown(URL):\n",
        "  raw_contents = request.urlopen(URL).read().decode('utf8')\n",
        "  #raw_contents = BeautifulSoup(raw_contents).get_text()\n",
        "  #raw_contents=nltk.word_tokenize(raw_contents)\n",
        "  lowers = re.findall(r'\\b[a-z]+', raw_contents)\n",
        "  unknowns = [w for w in lowers if w not in nltk.corpus.words.words()]\n",
        "  return set(unknowns)\n",
        "\n",
        "unknown('https://en.wikipedia.org/wiki/Government_College_of_Engineering,_Kannur')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c97ba2826f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknowns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0munknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://en.wikipedia.org/wiki/Government_College_of_Engineering,_Kannur'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-c97ba2826f4a>\u001b[0m in \u001b[0;36munknown\u001b[0;34m(URL)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#raw_contents=nltk.word_tokenize(raw_contents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b[a-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0munknowns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlowers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknowns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-c97ba2826f4a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#raw_contents=nltk.word_tokenize(raw_contents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b[a-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0munknowns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlowers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknowns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju72CP8uo9FU"
      },
      "source": [
        "22. Examine the results of processing the URL http://news.bbc.co.uk/ using the reg-\n",
        "ular expressions suggested above. You will see that there is still a fair amount of\n",
        "non-textual data there, particularly JavaScript commands. You may also find that\n",
        "sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn5Dy8FWK3Cb"
      },
      "source": [
        "unknown('https://www.bbc.com/news')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIvKTNp_o_j6"
      },
      "source": [
        "23. Are you able to write a regular expression to tokenize text in such a way that the\n",
        "word *don’t* is tokenized into *do* and *n’t*? Explain why this regular expression won’t\n",
        "work: « n't|\\w+ »."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUTt9n35N3XF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff014ada-aa3b-4aec-d3cd-b139844f5537"
      },
      "source": [
        "f = 'corpus.txt'\n",
        "file = open(f)\n",
        "raw = file.read()\n",
        "#pattern= r'(n\\'t|\\w+)'   #This will accept don and it matched with \\w+ and gets included as don in the list\n",
        "pattern = r'(\\w+(?:\\'t))'  #?: greedily selects 't also along with the dont\n",
        "set(re.findall(pattern,raw))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"don't\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX7K09r3pB7F"
      },
      "source": [
        "24. Try to write code to convert text into hAck3r, using regular expressions and\n",
        "substitution, where e → 3 , i → 1 , o → 0 , l → | , s → 5 , . → 5w33t! , ate → 8 . Normalize\n",
        "the text to lowercase before converting it. Add more substitutions of your own.\n",
        "Now try to map s to two different values: $ for word-initial s , and 5 for word-\n",
        "internal s ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZKHgZhYTJJn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4431fa7d-5ce4-4b51-bff5-bac0df54bc70"
      },
      "source": [
        "text='Eat Jin (originally Kim Seok-jin Eating) is a mukbang show made by Jin himself'\n",
        "text = text.lower()\n",
        "text = re.sub(r'ate', '8', text) \n",
        "text = re.sub(r'e', '3', text)         \n",
        "text = re.sub(r'i', '1', text)       \n",
        "text = re.sub(r'o', '0', text)     \n",
        "text = re.sub(r'l', '|', text)    \n",
        "text = re.sub(r'\\.', '5w33t!', text)\n",
        "text = re.sub(r'\\b(s)', '$', text)\n",
        "text = re.sub(r's$', '5', text)\n",
        "text = re.sub(r'\\.', '5w33t!', text)\n",
        "text = re.sub(r'n', '^', text)\n",
        "text = re.sub(r'w', 'vv', text)\n",
        "text = re.sub(r'g', '9', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3at j1^(0r191^a||y k1m $30k-j1^ 3at1^9) 1s a mukba^9 $h0vv mad3 by j1^ h1ms3|f'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2strbG7EpHvQ"
      },
      "source": [
        "25. Pig Latin is a simple transformation of English text. Each word of the text is\n",
        "converted as follows:  \n",
        "move any consonant (or consonant cluster) that appears at\n",
        "the start of the word to the end, then append ay, e.g., string → ingstray, idle →\n",
        "idleay (see [link text](http://en.wikipedia.org/wiki/Pig_Latin [link text](https://))).   \n",
        "a. Write a function to convert a word to Pig Latin.   \n",
        "b. Write code that converts text, instead of individual words.    \n",
        "c. Extend it further to preserve capitalization, to keep qu together (so that\n",
        "quiet becomes ietquay , for example), and to detect when y is used as a consonant (e.g., yellow ) versus a vowel (e.g., style )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9IF6WKPpG_V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e2b7ea7-ae37-4f57-bfc4-682693e97812"
      },
      "source": [
        "def pigLatin(word):\n",
        "  pattern= r'\\b([^aeiou]*)(\\w*)'       \n",
        "  rep=r'\\2\\1ay' #\\b()()   \\1 \\2 is converted to \\2 \\1 and ay is added\n",
        "  return re.sub(pattern,rep,word)\n",
        "  \n",
        "pigLatin('string')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ingstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b0QEHp3bIBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "573c324d-6db5-4d3b-944e-9afab45d164e"
      },
      "source": [
        "def pigLatin2(text):\n",
        "  word=nltk.word_tokenize(text)\n",
        "  wd=''\n",
        "  for w in word:\n",
        "    wd+=(pigLatin(w)+' ')\n",
        "   \n",
        "  print(wd)   \n",
        "pigLatin2('Kim Nam Joon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imKay amNay oonJay \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzIpirHhpWmT"
      },
      "source": [
        "**26. Download some text from a language that has vowel harmony (e.g., Hungarian),\n",
        "extract the vowel sequences of words, and create a vowel bigram table.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnZLFQ0pXrA"
      },
      "source": [
        "27. Python’s random module includes a function choice() which randomly chooses\n",
        "an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible\n",
        "characters, with the letter h being twice as frequent as the others. Write a generator\n",
        "expression that produces a sequence of 500 randomly chosen letters drawn from\n",
        "the string \"aehh \" , and put this expression inside a call to the ''.join() function,\n",
        "to concatenate them into one long string. You should get a result that looks like\n",
        "uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha . Use split()\n",
        "and join() again to normalize the whitespace in this string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaeHiL_o62vv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d6533e1c-dda6-4273-e657-47b29d24239f"
      },
      "source": [
        "import random\n",
        "stri=[]\n",
        "for i in range(500):\n",
        "    stri.append(random.choice(\"aehh \"))\n",
        "j = ''.join(stri)  \n",
        "j"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'h hehhhahh   ahahaeaeahheahaahhaah hee hahehheahehh  ehe ae hae hhhhheheheeah heh h hhhehehea he  hhea   aaahea hhhheh  a aea  ah ahhaehhahahhe  hee hhhe aaa  eah   ahh ae aaaheahhhahhea h  ehahhheahhahehhehhe ehaaahhhaheheh h  ae hhhhah ah h ahahhhaaaeaaaahhehh hh hhhhhhhee ahhe ahhaaaah eeahehhhah hahhhae heah e ehaea heaa  ha ehe aehe ah ahehhhahhhehheaehhehahhh aa  eeheha h e a h hahh  ehhhh   heahh ehahhh hhhheh  aeahhhee aheh hhhheeahe ehehe h ehha hhhh heh eaaaeahahhaeaaheeahhehhhh haahhe'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNiUksMh_kcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b9d0a6bf-fffe-463a-e7ce-103e7bf71473"
      },
      "source": [
        "' '.join(j.split())   #normalized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'h hehhhahh ahahaeaeahheahaahhaah hee hahehheahehh ehe ae hae hhhhheheheeah heh h hhhehehea he hhea aaahea hhhheh a aea ah ahhaehhahahhe hee hhhe aaa eah ahh ae aaaheahhhahhea h ehahhheahhahehhehhe ehaaahhhaheheh h ae hhhhah ah h ahahhhaaaeaaaahhehh hh hhhhhhhee ahhe ahhaaaah eeahehhhah hahhhae heah e ehaea heaa ha ehe aehe ah ahehhhahhhehheaehhehahhh aa eeheha h e a h hahh ehhhh heahh ehahhh hhhheh aeahhhee aheh hhhheeahe ehehe h ehha hhhh heh eaaaeahahhaeaaheeahhehhhh haahhe'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsMFSMudpaR9"
      },
      "source": [
        "28. Consider the numeric expressions in the following sentence from the MedLine\n",
        "Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53+/- 0.15% is three words? Or should we say that it’s a single compound word? Or should we say that it is actually nine words, since it’s read “four point five three, plus or minus fifteen percent”? Or should we say that it’s not a “real” word at all,since it wouldn’t appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNMUkawJA1_H"
      },
      "source": [
        "a. 4.53+/- 0.15% as a numeric expression can be intrepretted as 4.53+0.15% to 4.53-0.15% . ie  4.523 to 4.537   => 3 words  \n",
        "b. '4.53+/- 0.15%' can be considered as a single expression.  \n",
        "c. \"four point five three, plus or minus fifteen percent\" this could be said as nine words.  \n",
        "d. possibility of 4th argument also exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q1Wao6UpeRY"
      },
      "source": [
        "29. Readability measures are used to score the reading difficulty of a text, for the\n",
        "purposes of selecting texts of appropriate difficulty for language learners. Let us\n",
        "define μ w to be the average number of letters per word, and μ s to be the average\n",
        "number of words per sentence, in a given text. The Automated Readability Index\n",
        "(ARI) of the text is defined to be: 4.71 μ w + 0.5 μ s - 21.43. Compute the ARI score\n",
        "for various sections of the Brown Corpus, including section f (popular lore) and\n",
        "j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, whereas nltk.corpus.brown.sents() produces a sequence of\n",
        "sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRXPapJe_thx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "19b68a26-3ab6-4bd7-829c-176cd71b1eb8"
      },
      "source": [
        " from nltk.corpus import brown\n",
        "\n",
        "def mw(category):\n",
        "  wl = sum((len(w) for w in brown.words(categories=category)))\n",
        "  wn = len(brown.words(categories=category))\n",
        "  return wl/wn\n",
        "\n",
        "def ms(category):\n",
        "    sl = sum(len(s) for s in brown.sents(categories=category))\n",
        "    sn = len(brown.sents(categories=category))\n",
        "    return sl/sn\n",
        "\n",
        "def ari(category):\n",
        "    return 4.71 * mw(category) + 0.5 * ms(category) - 21.43\n",
        "\n",
        "for category in brown.categories():\n",
        "    print(category, ari(category))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adventure 4.0841684990890705\n",
            "belles_lettres 10.987652885621749\n",
            "editorial 9.471025332953673\n",
            "fiction 4.9104735321302115\n",
            "government 12.08430349501021\n",
            "hobbies 8.922356393630267\n",
            "humor 7.887805248319808\n",
            "learned 11.926007043317348\n",
            "lore 10.254756197101155\n",
            "mystery 3.8335518942055167\n",
            "news 10.176684595052684\n",
            "religion 10.203109907301261\n",
            "reviews 10.769699888473433\n",
            "romance 4.34922419804213\n",
            "science_fiction 4.978058336905399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxJEOEv2p5r9"
      },
      "source": [
        "30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
        "on each word. Do the same thing with the Lancaster Stemmer, and see if you ob-\n",
        "serve any differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHLPr9AWNrsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "3799e372-7151-48d2-e2c3-3c0c3ba2ca5c"
      },
      "source": [
        "raw=\"I’d like to ask all of you, What is your name? What excites you and makes your heart beat?\"\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "set([porter.stem(t) for t in tokens])     #more precise about the stem words. Replace with stem word effectievely     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',',\n",
              " '?',\n",
              " 'I',\n",
              " 'all',\n",
              " 'and',\n",
              " 'ask',\n",
              " 'beat',\n",
              " 'd',\n",
              " 'excit',\n",
              " 'heart',\n",
              " 'is',\n",
              " 'like',\n",
              " 'make',\n",
              " 'name',\n",
              " 'of',\n",
              " 'to',\n",
              " 'what',\n",
              " 'you',\n",
              " 'your',\n",
              " '’'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgR8jb4GPTSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "29ac2afd-b931-4cbd-db3f-cd18bb7e2197"
      },
      "source": [
        "set([lancaster.stem(t) for t in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',',\n",
              " '?',\n",
              " 'al',\n",
              " 'and',\n",
              " 'ask',\n",
              " 'beat',\n",
              " 'd',\n",
              " 'excit',\n",
              " 'heart',\n",
              " 'i',\n",
              " 'is',\n",
              " 'lik',\n",
              " 'mak',\n",
              " 'nam',\n",
              " 'of',\n",
              " 'to',\n",
              " 'what',\n",
              " 'yo',\n",
              " 'you',\n",
              " '’'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YneF8ETIp8ku"
      },
      "source": [
        "31. Define the variable saying to contain the list **[ 'After', 'all', 'is', 'said','and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']** . Process the list using a for **loop**, and store the result in a new list **lengths** . Hint: begin by assigning\n",
        "the empty list to lengths , using **lengths = [ ]** . Then each time through the loop,\n",
        "use **append( )** to add another length value to the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXw5TWMYecn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "624849f9-b9bd-4548-cd03-654eaa7487fb"
      },
      "source": [
        "l = [ 'After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
        "lengths=[]\n",
        "for word in l:\n",
        "  lengths.append(len(word))\n",
        "lengths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ9gqnRtqBqf"
      },
      "source": [
        "32. Define a variable **silly** to contain the string: **'newly formed bland ideas are inexpressible in an infuriating way'** . (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky’s famous\n",
        "nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now write code to perform the following tasks:  \n",
        "a. Split **silly** into a list of strings, one per word, using Python’s **split( )** operation, and save this to a variable called **bland**.  \n",
        "b. Extract the second letter of each word in **silly** and join them into a string, to\n",
        "get **'eoldrnnnna'** .  \n",
        "c. Combine the words in **bland** back into a single string, using **join()** . Make sure\n",
        "the words in the resulting string are separated with **whitespace**.  \n",
        "d. Print the words of **silly** in alphabetical order, one per line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPxKSHcgfFVA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "f1a5eacf-c714-4362-c03e-1e5408a44907"
      },
      "source": [
        "#a\n",
        "silly='newly formed bland ideas are inexpressible in an infuriating way'\n",
        "bland=silly.split()\n",
        "bland"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['newly',\n",
              " 'formed',\n",
              " 'bland',\n",
              " 'ideas',\n",
              " 'are',\n",
              " 'inexpressible',\n",
              " 'in',\n",
              " 'an',\n",
              " 'infuriating',\n",
              " 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DiXOKhSh7J1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60fd7ef1-3a2c-4a07-d98b-06e9de6864b5"
      },
      "source": [
        "#b\n",
        "w=' '\n",
        "for word in bland:\n",
        "  w+=word[1]\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' eoldrnnnna'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaiNf_gVh-Ia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64b105eb-2f44-4ca0-8455-48405c1a9dcc"
      },
      "source": [
        "#c\n",
        "' '.join(bland)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'newly formed bland ideas are inexpressible in an infuriating way'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LDWLIZTh__J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "fd7481ce-dd7b-425d-e439-47a9272f69cf"
      },
      "source": [
        "sorted(bland)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an',\n",
              " 'are',\n",
              " 'bland',\n",
              " 'formed',\n",
              " 'ideas',\n",
              " 'in',\n",
              " 'inexpressible',\n",
              " 'infuriating',\n",
              " 'newly',\n",
              " 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNRYJAMXqJWX"
      },
      "source": [
        "33. The **index( )** function can be used to look up items in sequences. For example, **'inexpressible'.index('e')** tells us the index of the first position of the letter e .  \n",
        "a. What happens when you look up a substring, e.g., **'inexpressible'.index('re')** ?  \n",
        "b. Define a variable words containing a list of words. Now use words.**index( )** to\n",
        "look up the position of an individual word.  \n",
        "c. Define a variable silly as in Exercise 32. Use the **index( )** function in combination with list slicing to build a list phrase consisting of all the words up to\n",
        "(but not including) **in** in s**illy** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLyvJgZHi2Ml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "93f985f6-d16c-4c7f-a314-96a9fd9c9344"
      },
      "source": [
        "#a\n",
        "print('a. index of \\'re\\': %d' %'inexpressible'.index('re'))\n",
        "#b\n",
        "words = ['a', 'list', 'of', 'words']\n",
        "print('b. index of \\'of\\': %d' %words.index('of'))\n",
        "#c\n",
        "print('c.')\n",
        "bland[:bland.index('in')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a. index of 're': 5\n",
            "b. index of 'of': 2\n",
            "c.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVRqg_41qM8s"
      },
      "source": [
        "34. Write code to convert nationality adjectives such as Canadian and Australian to\n",
        "their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_X3mdwpmPXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33ec8dea-c3d1-4288-b4a8-e731b0c5a4cf"
      },
      "source": [
        "repl = r'\\1a)'\n",
        "re.sub(r'(\\w+)(ian)', repl, 'Canadian')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Canada)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Qku9Vapgic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c0d28f9-99ee-4382-8f55-c8e872ad2070"
      },
      "source": [
        "re.sub(r'(\\w+)(?:an)', repl, 'Australian')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Australia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9rVMDq3qPfe"
      },
      "source": [
        "35. Read the LanguageLog post on phrases of the form as best as p can and as best p\n",
        "can, where p is a pronoun. Investigate this phenomenon with the help of a corpus\n",
        "and the findall() method for searching tokenized text described in Section 3.5.\n",
        "The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqGm8uhNmYj0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b30cafef-ee28-4b85-e7c0-4dd6f1c07350"
      },
      "source": [
        "text = \"\"\" I wil straight dispose, as best I can, th'inferiour Magistrate ...\n",
        "And I haue thrust my selfe into this maze, Happily to wiue and thriue, as best I may ...\n",
        "In fine, my life is that of a great schoolboy, getting into scrapes for the fun of it,\n",
        "and fighting my way out as best as I can!\n",
        "As best as she can she hides herself in the full sunlight\n",
        "\"\"\"\n",
        "re.findall(r'(?i)as best (?:as )?(?:I|we|you|he|she|they|it) can', text)   #(?i) any  case is matched"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['as best I can', 'as best as I can', 'As best as she can']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHfE5Cq-qQse"
      },
      "source": [
        "36. Study the *lolcat* version of the book of *Genesis*, accessible as *nltk.corpus.genesis.words('lolcat.txt')* , and the rules for converting text into *lolspeak* at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding *lolspeak* words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFy7HjFA9HjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "16acef78-9f69-44f9-f054-966de01d98ed"
      },
      "source": [
        "' '.join(nltk.corpus.genesis.words('lolcat.txt')[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Oh hai . In teh beginnin Ceiling Cat maded teh skiez An da Urfs , but he did not eated dem . Da Urfs no had shapez An haded dark face , An Ceiling Cat rode invisible bike over teh waterz . An Ceiling Cat sayed light Day An dark no Day . It were FURST !!! 1 An Ceiling Cat sayed , i can has teh firmmint wich iz funny bibel naim 4 ceiling , so wuz teh twoth day . An Ceiling Cat called no waterz urth and waters oshun . Iz good . An so teh threeth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf3fecjVtk3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a059f8c8-6764-40cb-a063-ac4ad570a90e"
      },
      "source": [
        "text = 'sight kite dude over kitty little'\n",
        "# just implement some easy-to-check rules\n",
        "text = re.sub(r'ight', 'iet', text)                             # ight -> iet\n",
        "text = re.sub(r'\\bdude\\b', 'dood', text)                        # dude -> dood\n",
        "text = re.sub(r'([^aeiou])(e)\\b', r'\\2\\1', text)       # exchange the consonant and the endding 'e'\n",
        "text = re.sub(r'er\\b', 'ah', text)                              # -er -> -ah\n",
        "text = re.sub(r'y\\b', 'eh', text)                               # -y -> -eh\n",
        "text = re.sub(r'le\\b', 'el', text)                              # -le -> -el\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'siet kiet dood ovah kitteh littel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbhLqaASqVy8"
      },
      "source": [
        "37. Read about the *re.sub()* function for string substitution using regular expressions, using *help(re.sub)* and by consulting the further readings for this chapter.\n",
        "Use *re.sub* in writing code to remove HTML tags from an HTML file, and to\n",
        "normalize whitespace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b04gpeWWuMK5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "4c4ce7fa-da82-4f2a-fed8-8af3ac4c186f"
      },
      "source": [
        "file = open('we.html').read()\n",
        "file = re.sub(r'<.*>', '', file)\n",
        "file = re.sub(r'\\s+', ' ', file)\n",
        "file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Earth Hour is a worldwide movement organized by the World Wide Fund for Nature(WWF).The event is held annually encouraging individuals,communities ,and buisness to turn off non-essential electric lights for one hour, from 8.30 to 9.30 pm on a specific day towards the end of March, as a symbol of commitment to the Planet. Lets move on... Starting as a symbolic lights out event in Sydney in 2007, Earth Hour is now the world's largest grassrootts movement for the environment, iinspiring millilons of people to take action for our planet and nurture. /* * multi-line comment */ p{ line-height: 1.5em; } h1, h2, h4{ color: orange; font-weight: normal; line-height: 1.1em; margin: 0 0 .5em 0; } h3{ color: black; font-weight: normal; line-height: 1.1em; margin: 0 0 .5em 0; } h2{ font-size: 1.5em; font-weight:bold } h1{ font-size: 1.7em; text-align:center; line-height:5% } a{ color: black; text-decoration: none; } a:hover,a:active{ text-decoration: underline; }body{ font-family: arial; font-size: 80%; line-height: 1.2em; width: 100%; margin: 0; background: #eee; } #page{ margin: 20px; } #logo{ width: 35%; margin-top: 5px; font-family: georgia; display: inline-block; } #nav{ width: 60%; display: inline-block; text-align: right; float: right; } #nav ul{} #nav ul li{ display: inline-block; height: 62px; } #nav ul li a{ padding: 20px; background: orange; color: white; } #nav ul li a:hover{ background-color: #ffb424; box-shadow: 0px 1px 1px #666; } #nav ul li a:active{ background-color: #ff8f00; } #content1{ margin: 30px 0; background: white ; opacity:0.6; padding: 20px; clear: both; } #content1, ul li a{ box-shadow: 0px 1px 1px #999; } #content1 a{ display: inline-block; text-align: center; height: 5px; } #content1 a{ padding: 20px; background: orange; color: white; } #content1 a:hover{ background-color: #ffb424; box-shadow: 0px 1px 1px #666; } #content1 a:active{ background-color: #ff8f00; } #content2 { float:left; width:30%; margin: 30px 0; background: white; opacity:0.6; padding: 20px; clear: both; } #content2, ul li a{ box-shadow: 0px 1px 1px #999; } #content2 a{ display: inline-block; text-align: center; height: 5px; } #content2 a{ padding: 20px; background: orange; color: white; } #content2 a:hover{ background-color: #ffb424; box-shadow: 0px 1px 1px #666; } #content2 a:active{ background-color: #ff8f00; } #content3 { float:right; width:30%; margin: 30px 0; background: white; opacity:0.6; padding: 20px; clear: both; } #content3, ul li a{ box-shadow: 0px 1px 1px #999; } #content3 a{ display: inline-block; text-align: center; height: 5px; } #content3 a{ padding: 20px; background: orange; color: white; } #content3 a:hover{ background-color: #ffb424; box-shadow: 0px 1px 1px #666; } #content3 a:active{ background-color: #ff8f00; } #footer{ border-bottom: 1px #ccc solid; margin-bottom: 10px; } #footer p{ text-align: right; text-transform: uppercase; font-size: 80%; color: grey; } \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fbsbKLhqYVk"
      },
      "source": [
        "38.An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is split, then we have the string long-\\nterm .  \n",
        "a. Write a regular expression that identifies words that are hyphenated at a linebreak. The expression will need to include the *\\n* character.  \n",
        "b. Use re.sub() to remove the \\n character from these words.  \n",
        "c. How might you identify words that should not remain hyphenated once the newline is removed, e.g., 'encyclo-\\npedia' ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNAXbzYsCPx0"
      },
      "source": [
        "a. r'\\w+-\\n\\w+'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ1KyFQVuX33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7001ba64-5b0d-479c-9e49-eb9a203b930d"
      },
      "source": [
        "#b.\n",
        "ext='long-term'\n",
        "pattern = r'(\\w+-)(\\n)(\\w+)'\n",
        "re.findall(pattern, text)\n",
        "re.sub(pattern, r'\\1\\3', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'long-term'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBh0jZZPCLq3"
      },
      "source": [
        "c. Check whether the hyphenated word is in the word corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O2ldzDUqcYX"
      },
      "source": [
        "**39.Read the Wikipedia entry on Soundex. Implement this algorithm in Python.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY3CWkDXqenK"
      },
      "source": [
        "40. Obtain raw texts from two or more genres and compute their respective reading\n",
        "difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\n",
        "Rural News and ABC Science News ( nltk.corpus.abc ). Use Punkt to perform sen-\n",
        "tence segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwvLOPIyu7Of",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "479d1492-f7ab-473c-bb1b-e1d10b36a623"
      },
      "source": [
        "nltk.corpus.abc.fileids()\n",
        "\n",
        "def ari(fileid):\n",
        "    words = nltk.corpus.abc.words(fileids=fileid)\n",
        "    \n",
        "    text = nltk.corpus.abc.raw(fileids=fileid)\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    \n",
        "    word_number = len(words)\n",
        "    word_length = sum(len(w) for w in words)\n",
        "    miu_w = word_length / word_number\n",
        "\n",
        "    sent_length = sum(len(s.split()) for s in sents)\n",
        "    sent_number = len(sents)\n",
        "    miu_s = sent_length / sent_number\n",
        "    \n",
        "    ari = 4.71 * miu_w + 0.5 * miu_s - 21.43\n",
        "    return ari\n",
        "print(ari('rural.txt'))\n",
        "print(ari('science.txt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.66074843699441\n",
            "10.703963706930097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmNCJI_qhjq"
      },
      "source": [
        "41. Rewrite the following nested loop as a nested list comprehension:\n",
        "```\n",
        "words = ['attribution', 'confabulation', 'elocution','sequoia', 'tenacious', 'unidirectional']\n",
        "vsequences = set()\n",
        "for word in words:\n",
        "      vowels = []\n",
        "      for char in word:\n",
        "         if char in 'aeiou':\n",
        "            vowels.append(char)\n",
        "      vsequences.add(''.join(vowels))\n",
        "sorted(vsequences)\n",
        "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L49LOdtOvBnD"
      },
      "source": [
        "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
        "vsequences = [''.join(re.findall(r'[aeiou]', v)) for v in words]\n",
        "sorted(vsequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thI1lvegrB-e"
      },
      "source": [
        "***42*. Use WordNet to create a semantic index for a text collection. Extend the con-\n",
        "cordance search program in Example 3-1, indexing each word using the offset of\n",
        "its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\n",
        "of its ancestors in the hypernym hierarchy).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG3aOjB9rFKM"
      },
      "source": [
        "43. With the help of a multilingual corpus such as the Universal Declaration of\n",
        "Human Rights Corpus ( nltk.corpus.udhr ), along with NLTK’s frequency distri-\n",
        "bution and rank correlation functionality ( nltk.FreqDist , nltk.spearman_correla\n",
        "tion ), develop a system that guesses the language of a previously unseen text. For\n",
        "simplicity, work with a single character encoding and just a few languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsd7oxAovEnw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "15563d24-6f15-4f89-c970-be6da7a4b535"
      },
      "source": [
        "def guess_language(text):\n",
        "    candidate_language = ['English-Latin1', 'French_Francais-Latin1', \n",
        "                          'German_Deutsch-Latin1', 'Italian-Latin1', 'Spanish-Latin1']\n",
        "\n",
        "    fdist = nltk.FreqDist(lang for lang in candidate_language\n",
        "                               for w in text if w in nltk.corpus.udhr.words(lang))\n",
        "    return fdist\n",
        "\n",
        "french='''Je voudrais vous demander à tous, quel est votre nom? Qu'est-ce qui vous excite et fait battre votre cœur? Raconte-moi ton histoire. Je veux entendre ta voix et je veux entendre ta conviction. Peu importe qui vous êtes, d'où vous venez, la couleur de votre peau, votre identité de genre, parlez-vous.'''.split()\n",
        "english='I’d like to ask all of you, What is your name? What excites you and makes your heart beat? Tell me your story. I want to hear your voice, and I want to hear your conviction. No matter who you are, where you’re from, your skin color, your gender identity, just speak yourself.'.split()\n",
        "german='Ich möchte Sie alle fragen: Wie ist Ihr Name? Was reizt dich und lässt dein Herz schlagen? Erzähl mir deine Geschichte. Ich möchte deine Stimme hören und ich möchte deine Überzeugung hören. Egal wer du bist, woher du kommst, deine Hautfarbe, deine Geschlechtsidentität, sprich einfach selbst.'.split()\n",
        "italian='Vorrei chiedere a tutti voi, qual è il vostro nome? Cosa ti eccita e ti fa battere il cuore? Raccontami la tua storia. Voglio sentire la tua voce e voglio sentire la tua convinzione. Non importa chi sei, da dove vieni, il colore della tua pelle, la tua identità di genere, parla solo te stesso.'.split()\n",
        "\n",
        "print(guess_language(english).max())\n",
        "print(guess_language(french).max())\n",
        "print(guess_language(german).max())\n",
        "print(guess_language(italian).max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English-Latin1\n",
            "French_Francais-Latin1\n",
            "German_Deutsch-Latin1\n",
            "Italian-Latin1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsiifd9rH1h"
      },
      "source": [
        "**44. Write a program that processes a text and discovers cases where a word has been\n",
        "used with a novel sense. For each word, compute the WordNet similarity between\n",
        "all synsets of the word and all synsets of the words in its context. (Note that this\n",
        "is a crude approach; doing it well is a difficult, open research problem.)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2N22a4grK57"
      },
      "source": [
        "**45. Read the article on normalization of non-standard words (Sproat et al., 2001),\n",
        "and implement a similar system for text normalization.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaIwTSVZ6v1s"
      },
      "source": [
        "# **Chapter 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7chXacF62Ix"
      },
      "source": [
        "1. Search the Web for “spoof newspaper headlines,” to find such gems as: British\n",
        "Left Waffles on Falkland Islands, and Juvenile Court to Try Shooting Defendant.\n",
        "Manually tag these headlines to see whether knowledge of the part-of-speech tags\n",
        "removes the ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsTsZSN57vEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d50bffa5-baff-4014-83a4-ed7b0306357d"
      },
      "source": [
        "text=nltk.word_tokenize(\"Daniel Adams too Naughty, Not Getting Gifts from Santa this Year\")\n",
        "nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Daniel', 'NNP'),\n",
              " ('Adams', 'NNP'),\n",
              " ('too', 'RB'),\n",
              " ('Naughty', 'NNP'),\n",
              " (',', ','),\n",
              " ('Not', 'RB'),\n",
              " ('Getting', 'VBG'),\n",
              " ('Gifts', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('Santa', 'NNP'),\n",
              " ('this', 'DT'),\n",
              " ('Year', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlBy2jl8Uc0"
      },
      "source": [
        "**2. Working with someone else, take turns picking a word that can be either a noun\n",
        "or a verb (e.g., contest); the opponent has to predict which one is likely to be the\n",
        "most frequent in the Brown Corpus. Check the opponent’s prediction, and tally\n",
        "the score over several turns.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sacYSfCs63Op"
      },
      "source": [
        "3.Tokenize and tag the following sentence: They wind back the clock, while we\n",
        "chase after the wind. What different pronunciations and parts-of-speech are\n",
        "involved?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHrI11pZ9rv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "82dc457f-c33e-43f7-9f50-733ca72f432a"
      },
      "source": [
        "text=nltk.word_tokenize(\"They wind back the clock, while we chase after the wind\")\n",
        "nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('wind', 'VBP'),\n",
              " ('back', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('clock', 'NN'),\n",
              " (',', ','),\n",
              " ('while', 'IN'),\n",
              " ('we', 'PRP'),\n",
              " ('chase', 'VBP'),\n",
              " ('after', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('wind', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0J5sUUM8ul4"
      },
      "source": [
        "4. Review the mappings in Table 5-4. Discuss any other examples of mappings you\n",
        "can think of. What type of information do they map from and to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJzbtPi2hVoJ"
      },
      "source": [
        "1. A phone book maps from names to phone numbers.\n",
        "2. A map maps from GPS coordinates to a physical location; A schedule maps from times to events. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgZElGS48yZ7"
      },
      "source": [
        "5. Using the Python interpreter in interactive mode, experiment with the dictionary\n",
        "examples in this chapter. Create a dictionary d , and add some entries. What happens whether you try to access a non-existent entry, e.g., d['xyz'] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5j_HstS-jX9"
      },
      "source": [
        "dict1={\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd7R3bdz_Z67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "a832ed7d-c1aa-42dd-97b9-3a47b99054ae"
      },
      "source": [
        "dict1['xyz']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-b2cc1e996b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xyz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'xyz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKe6HDDk85GM"
      },
      "source": [
        "6.Try deleting an element from a dictionary d , using the syntax del d['abc'] . Check\n",
        "that the item was deleted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMEgybvu_fYn"
      },
      "source": [
        "del dict1[\"brand\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K87dkEC_9ew",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "230e982a-c089-4d64-f596-743816eba287"
      },
      "source": [
        "dict1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'Mustang', 'year': 1964}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay_W5BsS881q"
      },
      "source": [
        "7.Create two dictionaries, d1 and d2 , and add some entries to each. Now issue the\n",
        "command d1.update(d2) . What did this do? What might it be useful for?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgwZWBXYAPP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9aca9369-6ef8-4755-acd0-9d4fb64e7b02"
      },
      "source": [
        "dict2={\n",
        "  \"owner\": \"John\",\n",
        "  \"price\": \"$150 million\",\n",
        "}\n",
        "dict1.update(dict2)\n",
        "dict1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'Mustang', 'owner': 'John', 'price': '$150 million', 'year': 1964}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyr0X7vI8-ku"
      },
      "source": [
        "8. Create a dictionary e , to represent a single lexical entry for some word of your\n",
        "choice. Define keys such as headword , part-of-speech , sense , and example , and assign them suitable values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC7WxmEvBIeC"
      },
      "source": [
        "word=nltk.word_tokenize('wind')\n",
        "e={\n",
        "    'word':word,\n",
        "    ##'headword':,\n",
        "    'part-of-speech':nltk.pos_tag(word),\n",
        "    'sense':'the perceptible natural movement of the air, especially in the form of a current of air blowing from a particular direction.',\n",
        "    'example':'the wind howled about the building'\n",
        "   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkDLZcXgwljw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21d93a73-a316-4e46-f9a0-a4a1550ca045"
      },
      "source": [
        "e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'headword': [('wind', 'NN')], 'word': ['wind']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR9yNnU99C86"
      },
      "source": [
        "9. satisfy yourself that there are restrictions on the distribution of go and went, in\n",
        "the sense that they cannot be freely interchanged in the kinds of contexts illustrated\n",
        "in (3), Section 5.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZN9pn3_iDIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e8e0b7b8-a488-456d-b5a6-a70fd71070be"
      },
      "source": [
        "t1=nltk.word_tokenize('Go away!')\n",
        "t2=nltk.word_tokenize('He sometimes goes to the cafe.')\n",
        "t3=nltk.word_tokenize('All the cakes have gone.')\n",
        "t4=nltk.word_tokenize('We went on the excursion.')\n",
        "t=['Go','go','went','Went','goes','gone']\n",
        "#print (tag in nltk.pos_tag(t1))\n",
        "k=[nltk.pos_tag(t1),nltk.pos_tag(t2),nltk.pos_tag(t3),nltk.pos_tag(t4)]\n",
        "for i in k:\n",
        "  for (j,k) in i:\n",
        "    if j in t:\n",
        "      print(j+' has tag '+ k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go has tag VB\n",
            "goes has tag VBZ\n",
            "gone has tag VBN\n",
            "went has tag VBD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyPH6wsq9ICv"
      },
      "source": [
        "10. Train a unigram tagger and run it on some new text. Observe that some words\n",
        "are not assigned a tag. Why not?    \n",
        "\n",
        "ans: Many words have been assigned a tag of None , because they were not among the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD4EQQwihy8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15aa17aa-b4e9-49b0-fad1-cece96de810d"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
        "a='I would like to ask all of you. What is your name? What excites you and makes your heart beat? Tell me your story. I want to hear your voice, and I want to hear your conviction. No matter who you are, where you\\’re from, your skin colour, gender identity: speak yourself. Find your name, find your voice by speaking yourself.'\n",
        "a=nltk.word_tokenize(a)\n",
        "unigram_tagger.tag(a)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PPSS'),\n",
              " ('would', 'MD'),\n",
              " ('like', 'CS'),\n",
              " ('to', 'TO'),\n",
              " ('ask', 'VB'),\n",
              " ('all', 'ABN'),\n",
              " ('of', 'IN'),\n",
              " ('you', 'PPSS'),\n",
              " ('.', '.'),\n",
              " ('What', 'WDT'),\n",
              " ('is', 'BEZ'),\n",
              " ('your', 'PP$'),\n",
              " ('name', 'NN'),\n",
              " ('?', '.'),\n",
              " ('What', 'WDT'),\n",
              " ('excites', None),\n",
              " ('you', 'PPSS'),\n",
              " ('and', 'CC'),\n",
              " ('makes', 'VBZ'),\n",
              " ('your', 'PP$'),\n",
              " ('heart', 'NN'),\n",
              " ('beat', 'VBD'),\n",
              " ('?', '.'),\n",
              " ('Tell', 'VB'),\n",
              " ('me', 'PPO'),\n",
              " ('your', 'PP$'),\n",
              " ('story', 'NN'),\n",
              " ('.', '.'),\n",
              " ('I', 'PPSS'),\n",
              " ('want', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('hear', 'VB'),\n",
              " ('your', 'PP$'),\n",
              " ('voice', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('I', 'PPSS'),\n",
              " ('want', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('hear', 'VB'),\n",
              " ('your', 'PP$'),\n",
              " ('conviction', 'NN'),\n",
              " ('.', '.'),\n",
              " ('No', 'AT'),\n",
              " ('matter', 'NN'),\n",
              " ('who', 'WPS'),\n",
              " ('you', 'PPSS'),\n",
              " ('are', 'BER'),\n",
              " (',', ','),\n",
              " ('where', 'WRB'),\n",
              " ('you\\\\', None),\n",
              " ('’', None),\n",
              " ('re', None),\n",
              " ('from', 'IN'),\n",
              " (',', ','),\n",
              " ('your', 'PP$'),\n",
              " ('skin', None),\n",
              " ('colour', None),\n",
              " (',', ','),\n",
              " ('gender', None),\n",
              " ('identity', 'NN'),\n",
              " (':', ':'),\n",
              " ('speak', 'VB'),\n",
              " ('yourself', 'PPL'),\n",
              " ('.', '.'),\n",
              " ('Find', None),\n",
              " ('your', 'PP$'),\n",
              " ('name', 'NN'),\n",
              " (',', ','),\n",
              " ('find', 'VB'),\n",
              " ('your', 'PP$'),\n",
              " ('voice', 'NN'),\n",
              " ('by', 'IN'),\n",
              " ('speaking', 'VBG'),\n",
              " ('yourself', 'PPL'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Ga_km-CVxi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4eb04f42-be21-44f7-cf10-76d69b37d38c"
      },
      "source": [
        "unigram_tagger.evaluate(brown_tagged_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9349006503968017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuu9n59L9KlI"
      },
      "source": [
        "11. Learn about the affix tagger (type help(nltk.AffixTagger) ). Train an affix tagger\n",
        "and run it on some new text. Experiment with different settings for the affix length\n",
        "and the minimum word length. Discuss your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TAs6E_joa0X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da226410-bc91-4c02-97e7-936448ff3162"
      },
      "source": [
        "help(nltk.AffixTagger)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class AffixTagger in module nltk.tag.sequential:\n",
            "\n",
            "class AffixTagger(ContextTagger)\n",
            " |  A tagger that chooses a token's tag based on a leading or trailing\n",
            " |  substring of its word string.  (It is important to note that these\n",
            " |  substrings are not necessarily \"true\" morphological affixes).  In\n",
            " |  particular, a fixed-length substring of the word is looked up in a\n",
            " |  table, and the corresponding tag is returned.  Affix taggers are\n",
            " |  typically constructed by training them on a tagged corpus.\n",
            " |  \n",
            " |  Construct a new affix tagger.\n",
            " |  \n",
            " |  :param affix_length: The length of the affixes that should be\n",
            " |      considered during training and tagging.  Use negative\n",
            " |      numbers for suffixes.\n",
            " |  :param min_stem_length: Any words whose length is less than\n",
            " |      min_stem_length+abs(affix_length) will be assigned a\n",
            " |      tag of None by this tagger.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      AffixTagger\n",
            " |      ContextTagger\n",
            " |      SequentialBackoffTagger\n",
            " |      nltk.tag.api.TaggerI\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, train=None, model=None, affix_length=-3, min_stem_length=2, backoff=None, cutoff=0, verbose=False)\n",
            " |      :param context_to_tag: A dictionary mapping contexts to tags.\n",
            " |      :param backoff: The backoff tagger that should be used for this tagger.\n",
            " |  \n",
            " |  context(self, tokens, index, history)\n",
            " |      :return: the context that should be used to look up the tag\n",
            " |          for the specified token; or None if the specified token\n",
            " |          should not be handled by this tagger.\n",
            " |      :rtype: (hashable)\n",
            " |  \n",
            " |  encode_json_obj(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  decode_json_obj(obj) from abc.ABCMeta\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  json_tag = 'nltk.tag.sequential.AffixTagger'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ContextTagger:\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __unicode__ = __str__(self, /)\n",
            " |  \n",
            " |  choose_tag(self, tokens, index, history)\n",
            " |      Decide which tag should be used for the specified token, and\n",
            " |      return that tag.  If this tagger is unable to determine a tag\n",
            " |      for the specified token, return None -- do not consult\n",
            " |      the backoff tagger.  This method should be overridden by\n",
            " |      subclasses of SequentialBackoffTagger.\n",
            " |      \n",
            " |      :rtype: str\n",
            " |      :type tokens: list\n",
            " |      :param tokens: The list of words that are being tagged.\n",
            " |      :type index: int\n",
            " |      :param index: The index of the word whose tag should be\n",
            " |          returned.\n",
            " |      :type history: list(str)\n",
            " |      :param history: A list of the tags for all words before *index*.\n",
            " |  \n",
            " |  size(self)\n",
            " |      :return: The number of entries in the table used by this\n",
            " |          tagger to map from contexts to tags.\n",
            " |  \n",
            " |  unicode_repr = __repr__(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from SequentialBackoffTagger:\n",
            " |  \n",
            " |  tag(self, tokens)\n",
            " |      Determine the most appropriate tag sequence for the given\n",
            " |      token sequence, and return a corresponding list of tagged\n",
            " |      tokens.  A tagged token is encoded as a tuple ``(token, tag)``.\n",
            " |      \n",
            " |      :rtype: list(tuple(str, str))\n",
            " |  \n",
            " |  tag_one(self, tokens, index, history)\n",
            " |      Determine an appropriate tag for the specified token, and\n",
            " |      return that tag.  If this tagger is unable to determine a tag\n",
            " |      for the specified token, then its backoff tagger is consulted.\n",
            " |      \n",
            " |      :rtype: str\n",
            " |      :type tokens: list\n",
            " |      :param tokens: The list of words that are being tagged.\n",
            " |      :type index: int\n",
            " |      :param index: The index of the word whose tag should be\n",
            " |          returned.\n",
            " |      :type history: list(str)\n",
            " |      :param history: A list of the tags for all words before *index*.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from SequentialBackoffTagger:\n",
            " |  \n",
            " |  backoff\n",
            " |      The backoff tagger for this tagger.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from nltk.tag.api.TaggerI:\n",
            " |  \n",
            " |  evaluate(self, gold)\n",
            " |      Score the accuracy of the tagger against the gold standard.\n",
            " |      Strip the tags from the gold standard text, retag it using\n",
            " |      the tagger, then compute the accuracy score.\n",
            " |      \n",
            " |      :type gold: list(list(tuple(str, str)))\n",
            " |      :param gold: The list of tagged sentences to score the tagger on.\n",
            " |      :rtype: float\n",
            " |  \n",
            " |  tag_sents(self, sentences)\n",
            " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
            " |      \n",
            " |          return [self.tag(sent) for sent in sentences]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT02ek4VpEPD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83f59e0a-5895-4cdf-a528-e3e202b68496"
      },
      "source": [
        "affix_tagger = nltk.AffixTagger(brown_tagged_sents,affix_length=-3, min_stem_length=2)\n",
        "a=nltk.word_tokenize('I would like to ask all of you. What is your name? What excites you and makes your heart beat? Tell me your story. I want to hear your voice, and I want to hear your conviction. No matter who you are, where you’re from, your skin colour, gender identity: speak yourself. Find your name, find your voice by speaking yourself.')\n",
        "affix_tagger.tag(a)\n",
        "affix_tagger.evaluate(brown_tagged_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25591224615629415"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-nNIsB3ruKu"
      },
      "source": [
        "12. Train a bigram tagger with no backoff tagger, and run it on some of the training\n",
        "data. Next, run it on some new data. What happens to the performance of the\n",
        "tagger? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQufcWNrAxwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13310b20-c34c-46cd-b3c4-76b0d67a81c2"
      },
      "source": [
        "tag1 = nltk.BigramTagger(brown_tagged_sents)\n",
        "tag1.tag(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PPSS'),\n",
              " ('would', 'MD'),\n",
              " ('like', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('ask', 'VB'),\n",
              " ('all', 'ABN'),\n",
              " ('of', 'IN'),\n",
              " ('you', 'PPO'),\n",
              " ('.', '.'),\n",
              " ('What', None),\n",
              " ('is', None),\n",
              " ('your', None),\n",
              " ('name', None),\n",
              " ('?', None),\n",
              " ('What', None),\n",
              " ('excites', None),\n",
              " ('you', None),\n",
              " ('and', None),\n",
              " ('makes', None),\n",
              " ('your', None),\n",
              " ('heart', None),\n",
              " ('beat', None),\n",
              " ('?', None),\n",
              " ('Tell', None),\n",
              " ('me', None),\n",
              " ('your', None),\n",
              " ('story', None),\n",
              " ('.', None),\n",
              " ('I', None),\n",
              " ('want', None),\n",
              " ('to', None),\n",
              " ('hear', None),\n",
              " ('your', None),\n",
              " ('voice', None),\n",
              " (',', None),\n",
              " ('and', None),\n",
              " ('I', None),\n",
              " ('want', None),\n",
              " ('to', None),\n",
              " ('hear', None),\n",
              " ('your', None),\n",
              " ('conviction', None),\n",
              " ('.', None),\n",
              " ('No', None),\n",
              " ('matter', None),\n",
              " ('who', None),\n",
              " ('you', None),\n",
              " ('are', None),\n",
              " (',', None),\n",
              " ('where', None),\n",
              " ('you', None),\n",
              " ('’', None),\n",
              " ('re', None),\n",
              " ('from', None),\n",
              " (',', None),\n",
              " ('your', None),\n",
              " ('skin', None),\n",
              " ('colour', None),\n",
              " (',', None),\n",
              " ('gender', None),\n",
              " ('identity', None),\n",
              " (':', None),\n",
              " ('speak', None),\n",
              " ('yourself', None),\n",
              " ('.', None),\n",
              " ('Find', None),\n",
              " ('your', None),\n",
              " ('name', None),\n",
              " (',', None),\n",
              " ('find', None),\n",
              " ('your', None),\n",
              " ('voice', None),\n",
              " ('by', None),\n",
              " ('speaking', None),\n",
              " ('yourself', None),\n",
              " ('.', None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myPJj5oqComy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd71a825-6fe6-4adc-9475-e1e0b5052ed3"
      },
      "source": [
        "tag1.evaluate(brown_tagged_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7892972929967977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJZTrN3grzPb"
      },
      "source": [
        "13. We can use a dictionary to specify the values to be substituted into a formatting\n",
        "string. Read Python’s library documentation for formatting strings (http://docs.python.org/lib/typesseq-strings.html) and use this method to display today’s date in\n",
        "two different formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EweHuWZu7G8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "88de9afa-973d-40fe-9f06-1533e7e15f46"
      },
      "source": [
        "import datetime\n",
        "today = datetime.date.today()\n",
        "print(today.strftime(\"%b %d %Y \"))\n",
        "print(today.strftime('%m/%d/%Y'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oct 24 2020 \n",
            "10/24/2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYCcrX8fr7cD"
      },
      "source": [
        "14. Use sorted() and set() to get a sorted list of tags used in the Brown Corpus,\n",
        "removing duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBn7nbxJFu0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d5d1821-0f01-4b84-9cd5-10bc5899cb76"
      },
      "source": [
        "k=nltk.corpus.brown.tagged_words()\n",
        "sorted(set(tag for (words,tag) in k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'\",\n",
              " \"''\",\n",
              " '(',\n",
              " '(-HL',\n",
              " ')',\n",
              " ')-HL',\n",
              " '*',\n",
              " '*-HL',\n",
              " '*-NC',\n",
              " '*-TL',\n",
              " ',',\n",
              " ',-HL',\n",
              " ',-NC',\n",
              " ',-TL',\n",
              " '--',\n",
              " '---HL',\n",
              " '.',\n",
              " '.-HL',\n",
              " '.-NC',\n",
              " '.-TL',\n",
              " ':',\n",
              " ':-HL',\n",
              " ':-TL',\n",
              " 'ABL',\n",
              " 'ABN',\n",
              " 'ABN-HL',\n",
              " 'ABN-NC',\n",
              " 'ABN-TL',\n",
              " 'ABX',\n",
              " 'AP',\n",
              " 'AP$',\n",
              " 'AP+AP-NC',\n",
              " 'AP-HL',\n",
              " 'AP-NC',\n",
              " 'AP-TL',\n",
              " 'AT',\n",
              " 'AT-HL',\n",
              " 'AT-NC',\n",
              " 'AT-TL',\n",
              " 'AT-TL-HL',\n",
              " 'BE',\n",
              " 'BE-HL',\n",
              " 'BE-TL',\n",
              " 'BED',\n",
              " 'BED*',\n",
              " 'BED-NC',\n",
              " 'BEDZ',\n",
              " 'BEDZ*',\n",
              " 'BEDZ-HL',\n",
              " 'BEDZ-NC',\n",
              " 'BEG',\n",
              " 'BEM',\n",
              " 'BEM*',\n",
              " 'BEM-NC',\n",
              " 'BEN',\n",
              " 'BEN-TL',\n",
              " 'BER',\n",
              " 'BER*',\n",
              " 'BER*-NC',\n",
              " 'BER-HL',\n",
              " 'BER-NC',\n",
              " 'BER-TL',\n",
              " 'BEZ',\n",
              " 'BEZ*',\n",
              " 'BEZ-HL',\n",
              " 'BEZ-NC',\n",
              " 'BEZ-TL',\n",
              " 'CC',\n",
              " 'CC-HL',\n",
              " 'CC-NC',\n",
              " 'CC-TL',\n",
              " 'CC-TL-HL',\n",
              " 'CD',\n",
              " 'CD$',\n",
              " 'CD-HL',\n",
              " 'CD-NC',\n",
              " 'CD-TL',\n",
              " 'CD-TL-HL',\n",
              " 'CS',\n",
              " 'CS-HL',\n",
              " 'CS-NC',\n",
              " 'CS-TL',\n",
              " 'DO',\n",
              " 'DO*',\n",
              " 'DO*-HL',\n",
              " 'DO+PPSS',\n",
              " 'DO-HL',\n",
              " 'DO-NC',\n",
              " 'DO-TL',\n",
              " 'DOD',\n",
              " 'DOD*',\n",
              " 'DOD*-TL',\n",
              " 'DOD-NC',\n",
              " 'DOZ',\n",
              " 'DOZ*',\n",
              " 'DOZ*-TL',\n",
              " 'DOZ-HL',\n",
              " 'DOZ-TL',\n",
              " 'DT',\n",
              " 'DT$',\n",
              " 'DT+BEZ',\n",
              " 'DT+BEZ-NC',\n",
              " 'DT+MD',\n",
              " 'DT-HL',\n",
              " 'DT-NC',\n",
              " 'DT-TL',\n",
              " 'DTI',\n",
              " 'DTI-HL',\n",
              " 'DTI-TL',\n",
              " 'DTS',\n",
              " 'DTS+BEZ',\n",
              " 'DTS-HL',\n",
              " 'DTX',\n",
              " 'EX',\n",
              " 'EX+BEZ',\n",
              " 'EX+HVD',\n",
              " 'EX+HVZ',\n",
              " 'EX+MD',\n",
              " 'EX-HL',\n",
              " 'EX-NC',\n",
              " 'FW-*',\n",
              " 'FW-*-TL',\n",
              " 'FW-AT',\n",
              " 'FW-AT+NN-TL',\n",
              " 'FW-AT+NP-TL',\n",
              " 'FW-AT-HL',\n",
              " 'FW-AT-TL',\n",
              " 'FW-BE',\n",
              " 'FW-BER',\n",
              " 'FW-BEZ',\n",
              " 'FW-CC',\n",
              " 'FW-CC-TL',\n",
              " 'FW-CD',\n",
              " 'FW-CD-TL',\n",
              " 'FW-CS',\n",
              " 'FW-DT',\n",
              " 'FW-DT+BEZ',\n",
              " 'FW-DTS',\n",
              " 'FW-HV',\n",
              " 'FW-IN',\n",
              " 'FW-IN+AT',\n",
              " 'FW-IN+AT-T',\n",
              " 'FW-IN+AT-TL',\n",
              " 'FW-IN+NN',\n",
              " 'FW-IN+NN-TL',\n",
              " 'FW-IN+NP-TL',\n",
              " 'FW-IN-TL',\n",
              " 'FW-JJ',\n",
              " 'FW-JJ-NC',\n",
              " 'FW-JJ-TL',\n",
              " 'FW-JJR',\n",
              " 'FW-JJT',\n",
              " 'FW-NN',\n",
              " 'FW-NN$',\n",
              " 'FW-NN$-TL',\n",
              " 'FW-NN-NC',\n",
              " 'FW-NN-TL',\n",
              " 'FW-NN-TL-NC',\n",
              " 'FW-NNS',\n",
              " 'FW-NNS-NC',\n",
              " 'FW-NNS-TL',\n",
              " 'FW-NP',\n",
              " 'FW-NP-TL',\n",
              " 'FW-NPS',\n",
              " 'FW-NPS-TL',\n",
              " 'FW-NR',\n",
              " 'FW-NR-TL',\n",
              " 'FW-OD-NC',\n",
              " 'FW-OD-TL',\n",
              " 'FW-PN',\n",
              " 'FW-PP$',\n",
              " 'FW-PP$-NC',\n",
              " 'FW-PP$-TL',\n",
              " 'FW-PPL',\n",
              " 'FW-PPL+VBZ',\n",
              " 'FW-PPO',\n",
              " 'FW-PPO+IN',\n",
              " 'FW-PPS',\n",
              " 'FW-PPSS',\n",
              " 'FW-PPSS+HV',\n",
              " 'FW-QL',\n",
              " 'FW-RB',\n",
              " 'FW-RB+CC',\n",
              " 'FW-RB-TL',\n",
              " 'FW-TO+VB',\n",
              " 'FW-UH',\n",
              " 'FW-UH-NC',\n",
              " 'FW-UH-TL',\n",
              " 'FW-VB',\n",
              " 'FW-VB-NC',\n",
              " 'FW-VB-TL',\n",
              " 'FW-VBD',\n",
              " 'FW-VBD-TL',\n",
              " 'FW-VBG',\n",
              " 'FW-VBG-TL',\n",
              " 'FW-VBN',\n",
              " 'FW-VBZ',\n",
              " 'FW-WDT',\n",
              " 'FW-WPO',\n",
              " 'FW-WPS',\n",
              " 'HV',\n",
              " 'HV*',\n",
              " 'HV+TO',\n",
              " 'HV-HL',\n",
              " 'HV-NC',\n",
              " 'HV-TL',\n",
              " 'HVD',\n",
              " 'HVD*',\n",
              " 'HVD-HL',\n",
              " 'HVG',\n",
              " 'HVG-HL',\n",
              " 'HVN',\n",
              " 'HVZ',\n",
              " 'HVZ*',\n",
              " 'HVZ-NC',\n",
              " 'HVZ-TL',\n",
              " 'IN',\n",
              " 'IN+IN',\n",
              " 'IN+PPO',\n",
              " 'IN-HL',\n",
              " 'IN-NC',\n",
              " 'IN-TL',\n",
              " 'IN-TL-HL',\n",
              " 'JJ',\n",
              " 'JJ$-TL',\n",
              " 'JJ+JJ-NC',\n",
              " 'JJ-HL',\n",
              " 'JJ-NC',\n",
              " 'JJ-TL',\n",
              " 'JJ-TL-HL',\n",
              " 'JJ-TL-NC',\n",
              " 'JJR',\n",
              " 'JJR+CS',\n",
              " 'JJR-HL',\n",
              " 'JJR-NC',\n",
              " 'JJR-TL',\n",
              " 'JJS',\n",
              " 'JJS-HL',\n",
              " 'JJS-TL',\n",
              " 'JJT',\n",
              " 'JJT-HL',\n",
              " 'JJT-NC',\n",
              " 'JJT-TL',\n",
              " 'MD',\n",
              " 'MD*',\n",
              " 'MD*-HL',\n",
              " 'MD+HV',\n",
              " 'MD+PPSS',\n",
              " 'MD+TO',\n",
              " 'MD-HL',\n",
              " 'MD-NC',\n",
              " 'MD-TL',\n",
              " 'NIL',\n",
              " 'NN',\n",
              " 'NN$',\n",
              " 'NN$-HL',\n",
              " 'NN$-TL',\n",
              " 'NN+BEZ',\n",
              " 'NN+BEZ-TL',\n",
              " 'NN+HVD-TL',\n",
              " 'NN+HVZ',\n",
              " 'NN+HVZ-TL',\n",
              " 'NN+IN',\n",
              " 'NN+MD',\n",
              " 'NN+NN-NC',\n",
              " 'NN-HL',\n",
              " 'NN-NC',\n",
              " 'NN-TL',\n",
              " 'NN-TL-HL',\n",
              " 'NN-TL-NC',\n",
              " 'NNS',\n",
              " 'NNS$',\n",
              " 'NNS$-HL',\n",
              " 'NNS$-NC',\n",
              " 'NNS$-TL',\n",
              " 'NNS$-TL-HL',\n",
              " 'NNS+MD',\n",
              " 'NNS-HL',\n",
              " 'NNS-NC',\n",
              " 'NNS-TL',\n",
              " 'NNS-TL-HL',\n",
              " 'NNS-TL-NC',\n",
              " 'NP',\n",
              " 'NP$',\n",
              " 'NP$-HL',\n",
              " 'NP$-TL',\n",
              " 'NP+BEZ',\n",
              " 'NP+BEZ-NC',\n",
              " 'NP+HVZ',\n",
              " 'NP+HVZ-NC',\n",
              " 'NP+MD',\n",
              " 'NP-HL',\n",
              " 'NP-NC',\n",
              " 'NP-TL',\n",
              " 'NP-TL-HL',\n",
              " 'NPS',\n",
              " 'NPS$',\n",
              " 'NPS$-HL',\n",
              " 'NPS$-TL',\n",
              " 'NPS-HL',\n",
              " 'NPS-NC',\n",
              " 'NPS-TL',\n",
              " 'NR',\n",
              " 'NR$',\n",
              " 'NR$-TL',\n",
              " 'NR+MD',\n",
              " 'NR-HL',\n",
              " 'NR-NC',\n",
              " 'NR-TL',\n",
              " 'NR-TL-HL',\n",
              " 'NRS',\n",
              " 'NRS-TL',\n",
              " 'OD',\n",
              " 'OD-HL',\n",
              " 'OD-NC',\n",
              " 'OD-TL',\n",
              " 'PN',\n",
              " 'PN$',\n",
              " 'PN+BEZ',\n",
              " 'PN+HVD',\n",
              " 'PN+HVZ',\n",
              " 'PN+MD',\n",
              " 'PN-HL',\n",
              " 'PN-NC',\n",
              " 'PN-TL',\n",
              " 'PP$',\n",
              " 'PP$$',\n",
              " 'PP$-HL',\n",
              " 'PP$-NC',\n",
              " 'PP$-TL',\n",
              " 'PPL',\n",
              " 'PPL-HL',\n",
              " 'PPL-NC',\n",
              " 'PPL-TL',\n",
              " 'PPLS',\n",
              " 'PPO',\n",
              " 'PPO-HL',\n",
              " 'PPO-NC',\n",
              " 'PPO-TL',\n",
              " 'PPS',\n",
              " 'PPS+BEZ',\n",
              " 'PPS+BEZ-HL',\n",
              " 'PPS+BEZ-NC',\n",
              " 'PPS+HVD',\n",
              " 'PPS+HVZ',\n",
              " 'PPS+MD',\n",
              " 'PPS-HL',\n",
              " 'PPS-NC',\n",
              " 'PPS-TL',\n",
              " 'PPSS',\n",
              " 'PPSS+BEM',\n",
              " 'PPSS+BER',\n",
              " 'PPSS+BER-N',\n",
              " 'PPSS+BER-NC',\n",
              " 'PPSS+BER-TL',\n",
              " 'PPSS+BEZ',\n",
              " 'PPSS+BEZ*',\n",
              " 'PPSS+HV',\n",
              " 'PPSS+HV-TL',\n",
              " 'PPSS+HVD',\n",
              " 'PPSS+MD',\n",
              " 'PPSS+MD-NC',\n",
              " 'PPSS+VB',\n",
              " 'PPSS-HL',\n",
              " 'PPSS-NC',\n",
              " 'PPSS-TL',\n",
              " 'QL',\n",
              " 'QL-HL',\n",
              " 'QL-NC',\n",
              " 'QL-TL',\n",
              " 'QLP',\n",
              " 'RB',\n",
              " 'RB$',\n",
              " 'RB+BEZ',\n",
              " 'RB+BEZ-HL',\n",
              " 'RB+BEZ-NC',\n",
              " 'RB+CS',\n",
              " 'RB-HL',\n",
              " 'RB-NC',\n",
              " 'RB-TL',\n",
              " 'RBR',\n",
              " 'RBR+CS',\n",
              " 'RBR-NC',\n",
              " 'RBT',\n",
              " 'RN',\n",
              " 'RP',\n",
              " 'RP+IN',\n",
              " 'RP-HL',\n",
              " 'RP-NC',\n",
              " 'RP-TL',\n",
              " 'TO',\n",
              " 'TO+VB',\n",
              " 'TO-HL',\n",
              " 'TO-NC',\n",
              " 'TO-TL',\n",
              " 'UH',\n",
              " 'UH-HL',\n",
              " 'UH-NC',\n",
              " 'UH-TL',\n",
              " 'VB',\n",
              " 'VB+AT',\n",
              " 'VB+IN',\n",
              " 'VB+JJ-NC',\n",
              " 'VB+PPO',\n",
              " 'VB+RP',\n",
              " 'VB+TO',\n",
              " 'VB+VB-NC',\n",
              " 'VB-HL',\n",
              " 'VB-NC',\n",
              " 'VB-TL',\n",
              " 'VBD',\n",
              " 'VBD-HL',\n",
              " 'VBD-NC',\n",
              " 'VBD-TL',\n",
              " 'VBG',\n",
              " 'VBG+TO',\n",
              " 'VBG-HL',\n",
              " 'VBG-NC',\n",
              " 'VBG-TL',\n",
              " 'VBN',\n",
              " 'VBN+TO',\n",
              " 'VBN-HL',\n",
              " 'VBN-NC',\n",
              " 'VBN-TL',\n",
              " 'VBN-TL-HL',\n",
              " 'VBN-TL-NC',\n",
              " 'VBZ',\n",
              " 'VBZ-HL',\n",
              " 'VBZ-NC',\n",
              " 'VBZ-TL',\n",
              " 'WDT',\n",
              " 'WDT+BER',\n",
              " 'WDT+BER+PP',\n",
              " 'WDT+BEZ',\n",
              " 'WDT+BEZ-HL',\n",
              " 'WDT+BEZ-NC',\n",
              " 'WDT+BEZ-TL',\n",
              " 'WDT+DO+PPS',\n",
              " 'WDT+DOD',\n",
              " 'WDT+HVZ',\n",
              " 'WDT-HL',\n",
              " 'WDT-NC',\n",
              " 'WP$',\n",
              " 'WPO',\n",
              " 'WPO-NC',\n",
              " 'WPO-TL',\n",
              " 'WPS',\n",
              " 'WPS+BEZ',\n",
              " 'WPS+BEZ-NC',\n",
              " 'WPS+BEZ-TL',\n",
              " 'WPS+HVD',\n",
              " 'WPS+HVZ',\n",
              " 'WPS+MD',\n",
              " 'WPS-HL',\n",
              " 'WPS-NC',\n",
              " 'WPS-TL',\n",
              " 'WQL',\n",
              " 'WQL-TL',\n",
              " 'WRB',\n",
              " 'WRB+BER',\n",
              " 'WRB+BEZ',\n",
              " 'WRB+BEZ-TL',\n",
              " 'WRB+DO',\n",
              " 'WRB+DOD',\n",
              " 'WRB+DOD*',\n",
              " 'WRB+DOZ',\n",
              " 'WRB+IN',\n",
              " 'WRB+MD',\n",
              " 'WRB-HL',\n",
              " 'WRB-NC',\n",
              " 'WRB-TL',\n",
              " '``']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6S4K9lXsDAp"
      },
      "source": [
        "15. Write programs to process the Brown Corpus and find answers to the following\n",
        "questions:  \n",
        "a. Which nouns are more common in their plural form, rather than their singular\n",
        "form? (Only consider regular plurals, formed with the -s suffix.)  \n",
        "b. Which word has the greatest number of distinct tags? What are they, and what\n",
        "do they represent?  \n",
        "c. List tags in order of decreasing frequency. What do the 20 most frequent tags\n",
        "represent?  \n",
        "d. Which tags are nouns most commonly found after? What do these tags\n",
        "represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OT3I5AniJ40"
      },
      "source": [
        "#a\n",
        "sing_nouns = set(word.lower() for (word, tag) in nltk.corpus.brown.tagged_words() if tag == 'NN')  #singular Noun\n",
        "plur_nouns = set([word.lower() for (word, tag) in nltk.corpus.brown.tagged_words() if tag == 'NNS']) #plural noun\n",
        "cands = [n for n in sing_nouns if n + \"s\" in plur_nouns]    #check whether singular noun a candidate of plural nouns\n",
        "snfd = nltk.FreqDist(word.lower() for (word, _) in nltk.corpus.brown.tagged_words() if word in sing_nouns) #singular noun freqdist\n",
        "pnfd = nltk.FreqDist(word.lower() for (word, _) in nltk.corpus.brown.tagged_words()  if word in plur_nouns) #plural noun freqdist\n",
        "more_common_plur = [(pnfd[c + 's'], c + 's', snfd[c], c) for c in cands if pnfd[c + 's'] > snfd[c]] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KZXvGQSC4Ig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "a665ff2a-4c39-4f9e-e444-a5e7dd679da8"
      },
      "source": [
        "#\n",
        "sorted(more_common_plur, reverse = True)[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(943, 'years', 649, 'year'),\n",
              " (391, 'eyes', 119, 'eye'),\n",
              " (361, 'things', 331, 'thing'),\n",
              " (312, 'members', 133, 'member'),\n",
              " (291, 'means', 199, 'mean'),\n",
              " (269, 'words', 261, 'word'),\n",
              " (204, 'students', 109, 'student'),\n",
              " (193, 'minutes', 54, 'minute'),\n",
              " (188, 'months', 130, 'month'),\n",
              " (179, 'conditions', 89, 'condition'),\n",
              " (173, 'hours', 145, 'hour'),\n",
              " (169, 'miles', 42, 'mile'),\n",
              " (160, 'terms', 79, 'term'),\n",
              " (150, 'friends', 125, 'friend'),\n",
              " (138, 'methods', 137, 'method'),\n",
              " (125, 'sales', 44, 'sale'),\n",
              " (115, 'arms', 91, 'arm'),\n",
              " (106, 'leaders', 69, 'leader'),\n",
              " (103, 'elements', 52, 'element'),\n",
              " (102, 'factors', 71, 'factor')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY3byGS2ESKW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3d237480-e2e0-498b-ce8f-c48927cb1ec8"
      },
      "source": [
        "#b\n",
        "from collections import defaultdict     #create default dictionary for resolving error\n",
        "dt = nltk.ConditionalFreqDist(brown.tagged_words())\n",
        "tags = defaultdict(list)\n",
        "for w in set(brown.words()):\n",
        "    tags[len(dt[w])].append(w)\n",
        "\n",
        "max(tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeU-MQyrDkUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "94bfcded-fbd4-4c40-d76b-ccf96f15d4ee"
      },
      "source": [
        "tags[12]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['that']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSlPqeB8LGGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "19dda894-9c3f-45ca-b4ab-1c794da886ed"
      },
      "source": [
        "\n",
        "dt['that']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'CS': 6419,\n",
              "          'CS-HL': 1,\n",
              "          'CS-NC': 2,\n",
              "          'DT': 1975,\n",
              "          'DT-NC': 6,\n",
              "          'NIL': 1,\n",
              "          'QL': 54,\n",
              "          'WPO': 135,\n",
              "          'WPO-NC': 1,\n",
              "          'WPS': 1638,\n",
              "          'WPS-HL': 2,\n",
              "          'WPS-NC': 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_0BTO0dLTwM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "ac5cc97d-3103-45f1-c49b-8acad69493b1"
      },
      "source": [
        "for tag in dt['that']:\n",
        "    nltk.help.brown_tagset(tag)      #another method is also found"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CS: conjunction, subordinating\n",
            "    that as after whether before while like because if since for than altho\n",
            "    until so unless though providing once lest s'posin' till whereas\n",
            "    whereupon supposing tho' albeit then so's 'fore\n",
            "WPS: WH-pronoun, nominative\n",
            "    that who whoever whosoever what whatsoever\n",
            "DT: determiner/pronoun, singular\n",
            "    this each another that 'nother\n",
            "QL: qualifier, pre\n",
            "    well less very most so real as highly fundamentally even how much\n",
            "    remarkably somewhat more completely too thus ill deeply little overly\n",
            "    halfway almost impossibly far severly such ...\n",
            "WPO: WH-pronoun, accusative\n",
            "    whom that who\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "No matching tags found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLpbsyHaLrPB"
      },
      "source": [
        "#c \n",
        "tags = [tags for _, tags in brown.tagged_words()]\n",
        "ft = nltk.FreqDist(tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuG1_vZ9Ly4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a0d8bf23-d138-4c58-9845-9904628287bd"
      },
      "source": [
        "print(ft.most_common(100), end = '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638), (',', 58156), ('NNS', 55110), ('CC', 37718), ('RB', 36464), ('NP', 34476), ('VB', 33693), ('VBN', 29186), ('VBD', 26167), ('CS', 22143), ('PPS', 18253), ('VBG', 17893), ('PP$', 16872), ('TO', 14918), ('PPSS', 13802), ('CD', 13510), ('NN-TL', 13372), ('MD', 12431), ('PPO', 11181), ('BEZ', 10066), ('BEDZ', 9806), ('AP', 9522), ('DT', 8957), ('``', 8837), (\"''\", 8789), ('QL', 8735), ('VBZ', 7373), ('BE', 6360), ('RP', 6009), ('WDT', 5539), ('HVD', 4895), ('*', 4603), ('WRB', 4509), ('BER', 4379), ('JJ-TL', 4107), ('NP-TL', 4019), ('HV', 3928), ('WPS', 3924), ('--', 3405), ('BED', 3282), ('ABN', 3010), ('DTI', 2921), ('PN', 2573), ('NP$', 2565), ('BEN', 2470), ('DTS', 2435), ('HVZ', 2433), (')', 2273), ('(', 2264), ('NNS-TL', 2226), ('EX', 2164), ('JJR', 1958), ('OD', 1935), ('NR', 1566), (':', 1558), ('NN$', 1480), ('IN-TL', 1477), ('NN-HL', 1471), ('DO', 1353), ('NPS', 1275), ('PPL', 1233), ('RBR', 1182), ('DOD', 1047), ('JJT', 1005), ('CD-TL', 898), ('MD*', 866), ('AT-TL', 746), ('ABX', 730), ('BEG', 686), ('NNS-HL', 609), ('UH', 608), ('.-HL', 598), ('VBN-TL', 591), ('NP-HL', 517), ('IN-HL', 508), ('DO*', 485), ('PPSS+MD', 484), ('DOZ', 467), ('CD-HL', 444), ('PPS+BEZ', 430), ('DOD*', 402), ('JJ-HL', 396), ('NN$-TL', 361), ('JJS', 359), ('ABL', 357), ('PPLS', 345), ('AT-HL', 332), (\"'\", 317), ('NR-TL', 309), ('CC-TL', 307), ('FW-NN', 288), ('HVG', 281), ('WPO', 280), ('PPSS+BER', 278), ('PPSS+BEM', 270), ('QLP', 261)]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiGK0bhBL3jq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b045f0e-eaca-4329-c6e4-2ad224eec3ae"
      },
      "source": [
        "for tag, _ in ft.most_common(20):\n",
        "    nltk.help.brown_tagset(tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN: noun, singular, common\n",
            "    failure burden court fire appointment awarding compensation Mayor\n",
            "    interim committee fact effect airport management surveillance jail\n",
            "    doctor intern extern night weekend duty legislation Tax Office ...\n",
            "IN: preposition\n",
            "    of in for by considering to on among at through with under into\n",
            "    regarding than since despite according per before toward against as\n",
            "    after during including between without except upon out over ...\n",
            "AT: article\n",
            "    the an no a every th' ever' ye\n",
            "JJ: adjective\n",
            "    ecent over-all possible hard-fought favorable hard meager fit such\n",
            "    widespread outmoded inadequate ambiguous grand clerical effective\n",
            "    orderly federal foster general proportionate ...\n",
            ".: sentence terminator\n",
            "    . ? ; ! :\n",
            ",: comma\n",
            "    ,\n",
            "NNS: noun, plural, common\n",
            "    irregularities presentments thanks reports voters laws legislators\n",
            "    years areas adjustments chambers $100 bonds courts sales details raises\n",
            "    sessions members congressmen votes polls calls ...\n",
            "CC: conjunction, coordinating\n",
            "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
            "RB: adverb\n",
            "    only often generally also nevertheless upon together back newly no\n",
            "    likely meanwhile near then heavily there apparently yet outright fully\n",
            "    aside consistently specifically formally ever just ...\n",
            "NP: noun, singular, proper\n",
            "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
            "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
            "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
            "VB: verb, base: uninflected present, imperative or infinitive\n",
            "    investigate find act follow inure achieve reduce take remedy re-set\n",
            "    distribute realize disable feel receive continue place protect\n",
            "    eliminate elaborate work permit run enter force ...\n",
            "VBN: verb, past participle\n",
            "    conducted charged won received studied revised operated accepted\n",
            "    combined experienced recommended effected granted seen protected\n",
            "    adopted retarded notarized selected composed gotten printed ...\n",
            "VBD: verb, past tense\n",
            "    said produced took recommended commented urged found added praised\n",
            "    charged listed became announced brought attended wanted voted defeated\n",
            "    received got stood shot scheduled feared promised made ...\n",
            "CS: conjunction, subordinating\n",
            "    that as after whether before while like because if since for than altho\n",
            "    until so unless though providing once lest s'posin' till whereas\n",
            "    whereupon supposing tho' albeit then so's 'fore\n",
            "PPS: pronoun, personal, nominative, 3rd person singular\n",
            "    it he she thee\n",
            "VBG: verb, present participle or gerund\n",
            "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
            "    keeping getting picking entering voting warning making strengthening\n",
            "    setting neighboring attending participating moving ...\n",
            "PP$: determiner, possessive\n",
            "    our its his their my your her out thy mine thine\n",
            "TO: infinitival to\n",
            "    to t'\n",
            "PPSS: pronoun, personal, nominative, not 3rd person singular\n",
            "    they we I you ye thou you'uns\n",
            "CD: numeral, cardinal\n",
            "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
            "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
            "    ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSPBuhL_L7t7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "270b739d-2fb0-4e91-8c7d-4d0f6d7072e5"
      },
      "source": [
        "#d\n",
        "wtp = nltk.bigrams(brown.tagged_words())\n",
        "np = [a[1] for (a, b) in wtp if b[1].startswith('NN')]\n",
        "fd = nltk.FreqDist(np)\n",
        "prec_tags = [tag for (tag, _) in fd.most_common(20)]\n",
        "print(prec_tags, end = '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['AT', 'JJ', 'IN', 'NN', 'PP$', 'CC', 'CD', 'AP', 'DT', 'VBG', ',', 'VBN', '.', 'NN-TL', 'JJ-TL', 'VB', 'NP', 'NP-TL', 'CS', 'NP$']"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NlqkVGOMAzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ea063e0-7a9f-4ed8-850b-8bc9c674ef73"
      },
      "source": [
        "for tag in prec_tags:\n",
        "    nltk.help.brown_tagset(tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AT: article\n",
            "    the an no a every th' ever' ye\n",
            "JJ: adjective\n",
            "    ecent over-all possible hard-fought favorable hard meager fit such\n",
            "    widespread outmoded inadequate ambiguous grand clerical effective\n",
            "    orderly federal foster general proportionate ...\n",
            "IN: preposition\n",
            "    of in for by considering to on among at through with under into\n",
            "    regarding than since despite according per before toward against as\n",
            "    after during including between without except upon out over ...\n",
            "NN: noun, singular, common\n",
            "    failure burden court fire appointment awarding compensation Mayor\n",
            "    interim committee fact effect airport management surveillance jail\n",
            "    doctor intern extern night weekend duty legislation Tax Office ...\n",
            "PP$: determiner, possessive\n",
            "    our its his their my your her out thy mine thine\n",
            "CC: conjunction, coordinating\n",
            "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
            "CD: numeral, cardinal\n",
            "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
            "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
            "    ...\n",
            "AP: determiner/pronoun, post-determiner\n",
            "    many other next more last former little several enough most least only\n",
            "    very few fewer past same Last latter less single plenty 'nough lesser\n",
            "    certain various manye next-to-last particular final previous present\n",
            "    nuf\n",
            "DT: determiner/pronoun, singular\n",
            "    this each another that 'nother\n",
            "VBG: verb, present participle or gerund\n",
            "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
            "    keeping getting picking entering voting warning making strengthening\n",
            "    setting neighboring attending participating moving ...\n",
            ",: comma\n",
            "    ,\n",
            "VBN: verb, past participle\n",
            "    conducted charged won received studied revised operated accepted\n",
            "    combined experienced recommended effected granted seen protected\n",
            "    adopted retarded notarized selected composed gotten printed ...\n",
            ".: sentence terminator\n",
            "    . ? ; ! :\n",
            "No matching tags found.\n",
            "No matching tags found.\n",
            "VB: verb, base: uninflected present, imperative or infinitive\n",
            "    investigate find act follow inure achieve reduce take remedy re-set\n",
            "    distribute realize disable feel receive continue place protect\n",
            "    eliminate elaborate work permit run enter force ...\n",
            "NP: noun, singular, proper\n",
            "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
            "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
            "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
            "No matching tags found.\n",
            "CS: conjunction, subordinating\n",
            "    that as after whether before while like because if since for than altho\n",
            "    until so unless though providing once lest s'posin' till whereas\n",
            "    whereupon supposing tho' albeit then so's 'fore\n",
            "NP$: noun, singular, proper, genitive\n",
            "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
            "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
            "    Shea's Palmer's Arnold's Broglio's ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBu4YWe_sHw5"
      },
      "source": [
        "16. Explore the following issues that arise in connection with the lookup tagger:  \n",
        "a. What happens to the tagger performance for the various model sizes when a\n",
        "backoff tagger is omitted?  \n",
        "b. Consider the curve in Figure 5-4; suggest a good size for a lookup tagger that\n",
        "balances memory and performance. Can you come up with scenarios where it\n",
        "would be preferable to minimize memory usage, or to maximize performance\n",
        "with no regard for memory usage?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsCVgh4dsKDm"
      },
      "source": [
        "17. What is the upper limit of performance for a lookup tagger, assuming no limit\n",
        "to the size of its table? (Hint: write a program to work out what percentage of tokens\n",
        "of a word are assigned the most likely tag for that word, on average.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30hRREwGOODs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d255492e-f3b0-4a41-9e9d-00e3ccec9473"
      },
      "source": [
        "# Brown Corpus\n",
        "cfd = nltk.ConditionalFreqDist(brown.tagged_words())\n",
        "sum([tag.startswith(cfd[word].max()) for word, tag in brown.tagged_words()]) / len(brown.tagged_words())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9360054151251472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS4vHm-AsNFq"
      },
      "source": [
        "18. Generate some statistics for tagged data to answer the following questions:\n",
        "a. What proportion of word types are always assigned the same part-of-speech\n",
        "tag?\n",
        "b. How many words are ambiguous, in the sense that they appear with at least\n",
        "two tags?\n",
        "c. What percentage of word tokens in the Brown Corpus involve these ambiguous\n",
        "words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W4sefxqsPpc"
      },
      "source": [
        "19. The evaluate() method works out how accurately the tagger performs on this\n",
        "text. For example, if the supplied tagged text was [('the', 'DT'), ('dog',\n",
        "'NN')] and the tagger produced the output [('the', 'NN'), ('dog', 'NN')] , then\n",
        "the score would be 0.5 . Let’s try to figure out how the evaluation method works:  \n",
        "a. A tagger t takes a list of words as input, and produces a list of tagged words\n",
        "as output. However, t.evaluate() is given correctly tagged text as its only\n",
        "parameter. What must it do with this input before performing the tagging?  \n",
        "b. Once the tagger has created newly tagged text, how might the evaluate()\n",
        "method go about comparing it with the original tagged text and computing\n",
        "the accuracy score?  \n",
        "c. Now examine the source code to see how the method is implemented. Inspect\n",
        "nltk.tag.api.\\__file__ to discover the location of the source code, and open\n",
        "this file using an editor (be sure to use the api.py file and not the compiled\n",
        "api.pyc binary file)."
      ]
    }
  ]
}